{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a978e8",
   "metadata": {},
   "source": [
    "# Let's try an non-hierarchical RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import nle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del gym.Wrapper.__getattr__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1ac6e",
   "metadata": {},
   "source": [
    "Import other useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baefd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plyr\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7faf3",
   "metadata": {},
   "source": [
    "Select the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_ = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device_ = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b3fef",
   "metadata": {},
   "source": [
    "Adam with high `weight_decay` may push many parameters' values\n",
    "into denormalized fp mode, which is ultra slow on CPU (but not\n",
    "as bad on GPU), see the answer and a reply form *njuffa* in to\n",
    "this [stackoverflow](https://stackoverflow.com/questions/36781881)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1629ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.set_flush_denormal(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d81ea6",
   "metadata": {},
   "source": [
    "Load modified minihack tasks, which punish death by `-1` reward\n",
    "* see the base class [MiniHack](https://github.com/facebookresearch/minihack/blob/65fc16f0f321b00552ca37db8e5f850cbd369ae5/minihack/base.py#L131-L132) subclassed by `MiniHackNavigation` and `MiniHackRoom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nle_toolbox.utils.env.minihack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffdfdcb",
   "metadata": {},
   "source": [
    "* [MiniHack-HideNSeek-Big-v0](https://minihack.readthedocs.io/en/latest/envs/navigation/hidenseek.html)\n",
    "> ... the agent is spawned in a big room full of trees and clouds \\[, which\\] block the line of sight of the player\\[, \\] and a random monster (chosen to be more powerful than the agent). The agent, monsters and spells can pass through clouds unobstructed \\[but\\] cannot pass through trees. The goals is to make use of the environment features, avoid being seen by the monster and quickly run towards the goal.\n",
    "\n",
    "* [MiniHack-WoD-Hard-v0](https://minihack.readthedocs.io/en/latest/envs/skills/wod.html)\n",
    "> ... require mastering the usage of the wand of death (WoD). Zapping a WoD it in any direction fires a death ray which instantly kills almost any monster it hits. ... the WoD needs to be found first, only then the agent should enter the corridor with a monster (who is awake and hostile this time), kill it, and go to the staircase.\n",
    "\n",
    "* [MiniHack-LavaCross-v0](https://minihack.readthedocs.io/en/latest/envs/skills/lava_cross.html)\n",
    "> The agent can accomplish this by either levitating over it (via a potion of levitation or levitation boots) or freezing it (by zapping the wand of cold or playing the frost horn).\n",
    "\n",
    "* [MiniHack-Memento-F4-v0](https://minihack.readthedocs.io/en/latest/envs/navigation/memento.html)\n",
    "> The agent is presented with a prompt (in the form of a sleeping monster of a specific type), and then navigates along a corridor. At the end of the corridor the agent reaches a fork, and must choose a direction. One direction leads to a grid bug, which if killed terminates the episode with +1 reward. All other directions lead to failure through a invisible trap that terminates the episode when activated. The correct path is determined by the cue seen at the beginning of the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91212f6",
   "metadata": {},
   "source": [
    "We hide the NLE under several layers of wrappers. From the core to the shell:\n",
    "1. `ReplayToFile` handles seeding and logs the taken actions and seed into a file for later inspection and replay.\n",
    "\n",
    "2. `NLEObservationPatches` patches tty-screens, botched by the cr-lf misconfiguration of the NLE's tty term emulator and NetHacks displays (lf only).\n",
    "\n",
    "3. `NLEFeatureExtractor` adds extra features generated on-the-fly from the current NLE's observation.\n",
    "  * an ego-centric view of the specified radius into the dungeon map (`vicinity`)\n",
    "  * percentage strength (`NLE_BL_STR125`), converted to a `100`-base score, used by the game to compute extra strength bonuses.\n",
    "    * The strength stat in AD&D 2ed, upon which the mechanics of NetHack is based, comes in two ints: strength and percentage. The latter is applicable to **warrior classes** with **natural str** 18 and denotes `exceptional strength`, which confers extra chance-to-hit, damage, and chance to force locks or doors.\n",
    "\n",
    "\n",
    "4. ~`RecentHistory` keeps a brief log of actions taken in the environment (partially duplicates the functionality of the `Replay` wrapper).~\n",
    "\n",
    "5. `Chassis` handles skippable gui events that do not require a decision, such as collecting menu pages unless an interaction is required, fetching consecutive topline or log messages.\n",
    "\n",
    "6. `ObservationDictFilter` allow only the specified keys of the observation dict to get through\n",
    "\n",
    "7. `ActionMasker` computes the mask of action that are **forbidden** in the current game state (_gui_ or _play_)\n",
    "  * the mask is communicated through the observation dicts under the key `action_mask`\n",
    "\n",
    "8. `RecentMessageLog` keeps a log of a specified number of recent messages fetched by the upstream `Chassis` wrapper.\n",
    "  * the log is reported in `message_log` field of the observation dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.bot.chassis import get_wrapper\n",
    "\n",
    "from nle_toolbox.utils.replay import ReplayToFile, Replay\n",
    "\n",
    "from nle_toolbox.utils.env.wrappers import NLEObservationPatches\n",
    "from nle_toolbox.utils.env.wrappers import NLEFeatureExtractor\n",
    "from nle_toolbox.utils.env.wrappers import RecentHistory\n",
    "\n",
    "from nle_toolbox.bot.chassis import Chassis, ActionMasker, RecentMessageLog\n",
    "\n",
    "from nle_toolbox.utils.env.wrappers import ObservationDictFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7194a",
   "metadata": {},
   "source": [
    "The factory for collecting random exploration rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6fe6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minihack\n",
    "from nle import nethack\n",
    "# from nle_toolbox.utils import seeding\n",
    "\n",
    "DEFAULT_OBSERVATION_KEYS = tuple(\n",
    "    frozenset(nethack.OBSERVATION_DESC.keys())\n",
    "    - frozenset(('program_state', 'internal'))\n",
    ")\n",
    "\n",
    "def factory(\n",
    "    seed=None,\n",
    "    *,\n",
    "    config,\n",
    "    folder=None,\n",
    "    sticky=False,\n",
    "):\n",
    "    # creat the env instance and get the action mapping\n",
    "    env = gym.make(\n",
    "        config['id'],\n",
    "        # remove service fields 'internal' and 'program_state'\n",
    "        observation_keys=DEFAULT_OBSERVATION_KEYS,\n",
    "    )\n",
    "\n",
    "    # XXX ascii (char) to action\n",
    "    ctoa = {chr(a): j for j, a in enumerate(env.unwrapped.actions)}\n",
    "    atoc = tuple(map(chr, env.unwrapped.actions))\n",
    "\n",
    "    # provide seeding capabilities and full action tracking\n",
    "    if folder is None:\n",
    "        env = Replay(env, sticky=sticky)\n",
    "\n",
    "    else:\n",
    "        env = ReplayToFile(env, sticky=sticky, folder=folder, save_on='done')\n",
    "    env.seed(seed)\n",
    "\n",
    "    # patch bugged tty output\n",
    "    env = NLEObservationPatches(env)\n",
    "\n",
    "    # # log recent actions\n",
    "    # env = RecentHistory(\n",
    "    #     env,\n",
    "    #     n_recent=128,\n",
    "    #     map=lambda a: atoc[a],  # XXX atoc IS NOT a dict!\n",
    "    # )\n",
    "\n",
    "    # skippable gui abstraction layer. Bypassed if the action\n",
    "    #  space does not bind a SPACE action.\n",
    "    # XXX we can also use \\015 (ENTER) aside from \\040 (SPACE).\n",
    "    space = ctoa.get(' ', ctoa.get('\\015'))\n",
    "    env = Chassis(env, space=space, split=False)\n",
    "\n",
    "    # a feature extractor to potentially reduce the runtime complexity\n",
    "    # * ego-centric view, and properly handled \"exceptional strength\" stat (str125)\n",
    "    env = NLEFeatureExtractor(env, k=config['vicinity'])\n",
    "\n",
    "    # filter unused observation keys\n",
    "    # XXX this wrapper should be applied before any container-type\n",
    "    #  modifications of the NLE's observation space.\n",
    "    env = ObservationDictFilter(\n",
    "        env,\n",
    "        ## the map, bottom line stats and inventory\n",
    "        'glyphs',\n",
    "        # 'chars', 'colors', 'specials',\n",
    "        'blstats',\n",
    "        'inv_glyphs',\n",
    "        # 'inv_strs',\n",
    "        'inv_letters',\n",
    "        # 'inv_oclasses',\n",
    "\n",
    "        ## used for in-notebook rendering\n",
    "        'tty_chars', 'tty_colors', 'tty_cursor',\n",
    "\n",
    "        ## used by the GUI abstraction layer (Chassis)\n",
    "        # 'message', 'misc',\n",
    "\n",
    "        ## other fields and fields related to internal state\n",
    "        # 'screen_descriptions',  # 'internal', 'program_state',\n",
    "\n",
    "        ## extra features produced by the upstream wrappers\n",
    "        'vicinity',\n",
    "    )\n",
    "\n",
    "    # compute and action mask based on the current NLE mode: gui or play\n",
    "    env = ActionMasker(env)\n",
    "\n",
    "    # track the recent messages from NetHack (necessary because the game\n",
    "    #  may send multi-part messages)\n",
    "    # XXX the longest modal message log does not exceeed 21 lines\n",
    "    env = RecentMessageLog(env, n_recent=8)  # extra 8 x 256 bytes\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a3da8",
   "metadata": {},
   "source": [
    "A renderer for this **factory**\n",
    "\n",
    "     y  k  u  \n",
    "      \\ | /   \n",
    "    h - . - l \n",
    "      / | \\   \n",
    "     b  j  n  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7124c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "from time import sleep\n",
    "\n",
    "from nle_toolbox.utils.env.render import render as tty_render\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def ipynb_render(obs, clear=True, fps=None):\n",
    "    if fps is not None:\n",
    "        if clear:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        print(tty_render(**obs))\n",
    "        if fps > 0:\n",
    "            sleep(fps)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b66e2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab8ed8",
   "metadata": {},
   "source": [
    "Pick the environment and get its action space parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# navigation, 8 compass directions\n",
    "# env_id = 'MiniHack-Room-Ultimate-15x15-v1'\n",
    "# env_id = 'MiniHack-CorridorBattle-Dark-v1'\n",
    "# env_id = 'MiniHack-HideNSeek-Big-v1'\n",
    "\n",
    "# navigation with 12 actions (8 compass, wait, enter, esc, space)\n",
    "# env_id = 'MiniHack-Room-Ultimate-15x15-MoreActions-v0'\n",
    "env_id = 'MiniHack-CorridorBattle-Dark-MoreActions-v0'\n",
    "# env_id = 'MiniHack-HideNSeek-Big-MoreActions-v0'\n",
    "\n",
    "# memory and navigation\n",
    "# env_id = 'MiniHack-Memento-F4-v1'\n",
    "# env_id = 'MiniHack-Memento-Short-F2-v0'\n",
    "\n",
    "# skill acquistion (advanced) 85 actions\n",
    "# env_id = 'MiniHack-WoD-Hard-v0'\n",
    "# env_id = 'MiniHack-LavaCross-v0'\n",
    "\n",
    "# Full game (ultimate) 121 actions\n",
    "# env_id = 'NetHackChallenge-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e62e3d",
   "metadata": {},
   "source": [
    "The essential parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "with gym.make(env_id) as env:\n",
    "    pp.pprint(env.unwrapped.actions)\n",
    "    n_actions = env.action_space.n\n",
    "    # n_actions = 12  # 85, 121  # XXX make this an automatic setting dependent on the env\n",
    "\n",
    "embedding_dim = 16\n",
    "intermediate_size = 256\n",
    "n_vicinity = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca2e87",
   "metadata": {},
   "source": [
    "No intrinsic motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_null = {'cls': None}\n",
    "\n",
    "recipe_null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f5b3b",
   "metadata": {},
   "source": [
    "Intrinsic motivation via Random Network distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f742c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.models.motivation import RNDModule, RNDNetwork\n",
    "\n",
    "recipe_rnd = RNDModule.default_recipe(\n",
    "    RNDNetwork.default_recipe(\n",
    "        embedding_dim=embedding_dim,\n",
    "        intermediate_size=intermediate_size,\n",
    "        sizes=(\n",
    "            intermediate_size,\n",
    "        ),\n",
    "        k=n_vicinity,\n",
    "        bls=('hp', 'hunger', 'condition',),\n",
    "        act={\n",
    "            'num_embeddings': n_actions,\n",
    "            'embedding_dim': embedding_dim,\n",
    "        },\n",
    "    ),\n",
    "    root=False,\n",
    ")\n",
    "\n",
    "recipe_rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2018538",
   "metadata": {},
   "source": [
    "Intrinsic motivation based on impact driven exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.models.motivation import RIDEModule, RIDEEmbedding\n",
    "\n",
    "recipe_ride = RIDEModule.default_recipe(\n",
    "    n_actions=n_actions,\n",
    "    embed=RIDEEmbedding.default_recipe(\n",
    "        embedding_dim=embedding_dim,\n",
    "        intermediate_size=intermediate_size,\n",
    "        k=n_vicinity,\n",
    "        bls=('hp', 'hunger', 'condition',),\n",
    "        h0=True,\n",
    "    ),\n",
    "    sizes=(256,),\n",
    "    bilinear=False,\n",
    "    flip=False,\n",
    ")\n",
    "\n",
    "recipe_ride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b98cdb",
   "metadata": {},
   "source": [
    "The recipe of the basic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.models.basic import NLENeuralAgent\n",
    "\n",
    "recipe_agent_basic = NLENeuralAgent.default_recipe(\n",
    "    n_actions=n_actions,\n",
    "    embedding_dim=embedding_dim,\n",
    "    intermediate_size=intermediate_size,\n",
    "    act_embedding_dim=None,  # default to `embedding_dim`\n",
    "    fin_embedding_dim=0,  # disable `fin` flag embedding\n",
    "#     core='lstm',\n",
    "    core='linear',\n",
    "    # it appears that more layers makes the agent learn better!\n",
    "    num_layers=1,\n",
    "    k=n_vicinity,\n",
    "    bls=('hp', 'hunger', 'condition',),\n",
    "    learn_tau=False,\n",
    ")\n",
    "\n",
    "recipe_agent_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452898c",
   "metadata": {},
   "source": [
    "A recipe for Highway Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.models.transformer import NLEHITNeuralAgent\n",
    "\n",
    "# highway xformer\n",
    "recipe_agent_hit = NLEHITNeuralAgent.default_recipe(\n",
    "    n_actions=n_actions,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_cls=8,\n",
    "    n_io=8,  # \\geq 4 is great, but \\leq 2 doesn't work\n",
    "    intermediate_size=64,\n",
    "    num_attention_heads=4,\n",
    "    head_size=16,\n",
    "    num_layers=1,  # 2 is gud, but can we use one layer?\n",
    "    k=n_vicinity,\n",
    "    bls=('hp', 'hunger', 'condition',),\n",
    "    learn_tau=False,\n",
    ")\n",
    "\n",
    "recipe_agent_hit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e82d85",
   "metadata": {},
   "source": [
    "Assemble the full recipe: agent and the motivator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = {\n",
    "    'agent': recipe_agent_basic,\n",
    "#     'agent': recipe_agent_hit,\n",
    "#     'motivator': recipe_rnd,\n",
    "    'motivator': recipe_ride,\n",
    "#     'motivator': recipe_null,\n",
    "}\n",
    "\n",
    "NLEAgent = NLENeuralAgent\n",
    "# NLEAgent = NLEHITNeuralAgent\n",
    "\n",
    "tags = (\n",
    "    'basic',  # 'hit',\n",
    "#     'rnd',\n",
    "    'ride',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61fbcd",
   "metadata": {},
   "source": [
    "intrinsic/extrinsic settings for the advantages, policy grads, critics, etc.\n",
    "and parameters of the fragment collectors, optimizers and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eaf83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO explain why a certain parameter is used\n",
    "#  and where it came from with the exact citation\n",
    "d_hyper_act = dict(\n",
    "    # weight of the ext/int rewards in the gae mix for the polgrad\n",
    "    f_alpha={'ext': 1., 'int': 0.5,},\n",
    "\n",
    "    # extrinsic/intrinsic reward PV discount\n",
    "    f_gamma={'ext': 0.99, 'int': 0.9,},\n",
    "    # XXX should we encourage more persistent exploration than exploitation?\n",
    "\n",
    "    # the GAE discount: interpolate between TD(0) and TD(\\infty)\n",
    "    # XXX good for RND, but what about other methods?\n",
    "    f_lambda={'ext': 0.96, 'int': 0.96,},\n",
    "    # XXX there are suggestions that a lambda close to one makes\n",
    "    #  the projected td(lambda) into a contraction, so we use a high\n",
    "    #  value of 0.96\n",
    "\n",
    "    # the coefficients in the loss for the both `ext` and\n",
    "    #  `int` critic terms, entropy, and policy grads\n",
    "    # XXX `-ve` maximizes, `+ve` minimizes\n",
    "    f_coefs={\n",
    "        'polgrad': -1.,\n",
    "        'entropy': -0.01,\n",
    "        'critic': {'ext': 0.5, 'int': 0.5,},\n",
    "    },\n",
    "\n",
    "    # the number of off-policy updates for GAE-A2C + impala or PPO+GAE\n",
    "    n_off_policy=3,\n",
    "    n_batch_size=None,  # the batch size for each pass (None -- full batch)\n",
    "    s_off_alg='ppo',\n",
    "\n",
    "    # the share of the runtime state's hx's gard\n",
    "    #  to be passed to `h0` between bptt fragments\n",
    "    f_h0_lerp=0.0,\n",
    "\n",
    "    f_lr=1e-3,\n",
    "\n",
    "    # the length of the truncated-bptt rollout for the actor/agent and motivator\n",
    "    n_length=100,\n",
    "\n",
    "    # gradient ell-2 norm clipping\n",
    "    f_grad_norm=5.0,  # float('inf'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d684d",
   "metadata": {},
   "source": [
    "The hyper parameters of the motivation modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71408692",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_hyper_mot = dict(\n",
    "    f_lr=1e-3,\n",
    "\n",
    "    # generic motivator hyper parameters\n",
    "    n_length=20,\n",
    "    \n",
    "    # the value to clip the motivator's rewards from above\n",
    "    #  (rews are currently non-negative)\n",
    "    f_rew_max=None,\n",
    "\n",
    "    # special hyper parameters\n",
    "    n_batches=1,  # the number of backprop passes thru a fragment\n",
    "    n_batch_size=None,  # the batch size for each pass (None -- full batch)\n",
    "    b_detach=False,\n",
    "\n",
    "    f_coefs={\n",
    "        'fwd': 10,\n",
    "        'inv': 0.1,\n",
    "    },\n",
    "\n",
    "    # gradient ell-2 norm clipping\n",
    "    # XXX clipping motivator's grad seems to adversely impact the performance\n",
    "    f_grad_norm=float('inf'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8f3c1",
   "metadata": {},
   "source": [
    "The fragmented a2c parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdb847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    mode='disabled',\n",
    "    project='nle-toolbox',\n",
    "    job_type='sandbox',\n",
    "    config=dict(\n",
    "        vicinity=n_vicinity,\n",
    "\n",
    "        # the number of envs to run simultaneously\n",
    "        n_batch=16,\n",
    "\n",
    "        # the total number of steps allotted to training (summed across all envs)\n",
    "#         n_total=2_592_000 * 6,\n",
    "        n_total=16000,\n",
    "\n",
    "        act=d_hyper_act,\n",
    "        mot=d_hyper_mot,\n",
    "\n",
    "        # also track the recipes\n",
    "        recipe=recipe,\n",
    "        checkpoint=None,\n",
    "\n",
    "        # the env id\n",
    "        id=env_id,\n",
    "\n",
    "        # duplicate the fragment lengths for ease of use in the dashboard\n",
    "        _n_act_length=d_hyper_act['n_length'],\n",
    "        _n_mot_length=d_hyper_mot['n_length'],\n",
    "    ),\n",
    "    tags=tags,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc8a21",
   "metadata": {},
   "source": [
    "* `linear` with small TxB batch (64x5) is more noisy, and produces less `stable` policy, but converges fastest\n",
    "* `linear` with larget batch (16x100) converges a bit slower, but produces more stable policy\n",
    "* `lstm` with 16x100 converges slowest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2709ece",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b592f56",
   "metadata": {},
   "source": [
    "### Redesigning the building blocks of the NLE featrue extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca3ffab",
   "metadata": {},
   "source": [
    "A module, which delays its structured input `T x ...` by the specified number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class DelayBy(nn.Module):\n",
    "    def __init__(self, n: int = 1):\n",
    "        assert n > 0\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, input: Any, hx: Any = None) -> tuple[Any, Any]:\n",
    "        # initialize the state: setup the pre-history\n",
    "        if hx is None:\n",
    "            hx = plyr.apply(lambda t: torch.zeros_like(t[-1:]), input)\n",
    "            if self.n > 1:\n",
    "                hx = plyr.tuply(torch.cat, *(hx,) * self.n, dim=0)\n",
    "\n",
    "        # splice the current input and the pre-history, then split\n",
    "        spliced = plyr.apply(torch.cat, hx, input, _star=False, dim=0)\n",
    "        return (\n",
    "            plyr.apply(lambda t, n=self.n: t[:-n], spliced),\n",
    "            plyr.apply(lambda t, n=self.n: t[-n:], spliced),\n",
    "        )\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'n={self.n}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1eb51",
   "metadata": {},
   "source": [
    "A clunky tool to split the parameters into biases and weights.\n",
    "\n",
    "Unlike `.bias` parameters, which **should not** be decayed, not every `.weight` parameter\n",
    "**should be** decayed. For example, learnt positional encodings can be seen as bias terms\n",
    "to the \"linear\" operation effected by an `nn.Embedding`, at the same time ordinary token\n",
    "embeddings should also not be decayed either, since they effectively serve as the input\n",
    "data representation.\n",
    "\n",
    "`nn.LayerNorm` layers perform within-layer normalization and then re-scale and translate\n",
    "it, meaning that their weight should be regularized to unit scales ant not zeros.\n",
    "\n",
    "Hence:\n",
    "* all biases, and weights of `nn.LayerNorm` and `nn.Embedding` should not be regularized\n",
    "by weight decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.models.transformer import HiT\n",
    "\n",
    "# XXX clean this up\n",
    "def split_parameters(module):\n",
    "    decay, no_decay, seen = [], [], set()\n",
    "    for prefix, mod in module.named_modules():\n",
    "        for name, par in mod.named_parameters(prefix='', recurse=False):\n",
    "            assert par not in seen\n",
    "            seen.add(par)\n",
    "\n",
    "            # always exclude bias terms\n",
    "            if name.startswith('bias'):\n",
    "                no_decay.append(par)\n",
    "\n",
    "            # but always decay dense linear weights\n",
    "            elif name.startswith('weight') and isinstance(\n",
    "                mod, (nn.Linear, nn.LSTM, nn.Bilinear)\n",
    "            ):\n",
    "                decay.append(par)\n",
    "\n",
    "            # yet never decay normalization translations and embedding representations\n",
    "            elif name.startswith('weight') and isinstance(\n",
    "                mod, (nn.LayerNorm, nn.Embedding)\n",
    "            ):\n",
    "                no_decay.append(par)\n",
    "\n",
    "            elif name in ('posemb', 'cls', 'iox') and isinstance(\n",
    "                mod, (HiT,)\n",
    "            ):\n",
    "                no_decay.append(par)\n",
    "\n",
    "            else:\n",
    "                no_decay.append(par)\n",
    "                # raise TypeError(f'Unrecognized parameter `{name}` in `{prefix}` {mod}.')\n",
    "\n",
    "    return decay, no_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd63e21",
   "metadata": {},
   "source": [
    "Build an agent from the recipe and reset the bias terms in its recurrent core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dcf27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "from nle_toolbox.utils.nn import rnn_reset_bias\n",
    "\n",
    "agent = NLEAgent(**wandb.config.recipe['agent'])\n",
    "agent.to(device_)\n",
    "\n",
    "agent.apply(rnn_reset_bias);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f5545",
   "metadata": {},
   "source": [
    "Init agent's optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ca95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay, no_decay = split_parameters(agent)\n",
    "\n",
    "# AdamW doesn't do what you expect it to do, Ivan! (although it\n",
    "#  correctly decouples the objective's grad and the ell-2 weight reg).\n",
    "#  See https://arxiv.org/abs/1711.05101.pdf\n",
    "agent.optim = torch.optim.AdamW([\n",
    "    dict(params=decay),\n",
    "    dict(params=no_decay, weight_decay=0.),\n",
    "], lr=wandb.config['act']['f_lr'], eps=1e-5, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3ca4c",
   "metadata": {},
   "source": [
    "Load an eariler checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb.config.checkpoint is not None:\n",
    "    ckpt = torch.load(wandb.config.checkpoint)\n",
    "    print(agent.load_state_dict(ckpt['agent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501aa1bd",
   "metadata": {},
   "source": [
    "Get the specified motivator: RIDE or RND, -- and prepare their optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61888499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.nn import ModuleDict\n",
    "\n",
    "recipe_m = wandb.config.recipe['motivator'].copy()\n",
    "cls_motivator = recipe_m.pop('cls')\n",
    "\n",
    "mot = None\n",
    "if cls_motivator is not None:\n",
    "    mot = {\n",
    "        str(c): c for c in (RIDEModule, RNDModule)\n",
    "    }[cls_motivator](**recipe_m)\n",
    "    mot.to(device_)\n",
    "\n",
    "    decay, no_decay = split_parameters(mot)\n",
    "    mot.optim = torch.optim.AdamW([\n",
    "        dict(params=decay),\n",
    "        dict(params=no_decay, weight_decay=0.),\n",
    "    ], lr=wandb.config['mot']['f_lr'], eps=1e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33113c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97638c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd53c7f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5264f",
   "metadata": {},
   "source": [
    "### Let's train an A2C agent in a capsule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c6fbc",
   "metadata": {},
   "source": [
    "An object to extract full episodes from their trajectory fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c77184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.tools import EpisodeExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28465837",
   "metadata": {},
   "source": [
    "A capulse for a learner agent and a launcher for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.capsule import launch, capsule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75236c1",
   "metadata": {},
   "source": [
    "Compute the policy gradient surrogate, the entropy and other loss components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea31c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import pyt_polgrad\n",
    "from nle_toolbox.utils.rl.engine import pyt_entropy\n",
    "from nle_toolbox.utils.rl.engine import pyt_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2ea52",
   "metadata": {},
   "source": [
    "We shall use GAE in policy gradients and returns for the critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.returns import pyt_ret_gae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7cbb1",
   "metadata": {},
   "source": [
    "Get the weighted sum of the leaves in one nested container with\n",
    "weights from another second container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad900c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator as op\n",
    "\n",
    "def reduce(values, weight=None):\n",
    "    flat = []\n",
    "    if weight is not None:\n",
    "        values = plyr.apply(op.mul, values, weight)\n",
    "\n",
    "    plyr.apply(flat.append, values)\n",
    "    return sum(flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1397a",
   "metadata": {},
   "source": [
    "A function to compute the targets (GAE, returns) for policy grads and critic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c04614",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pg_targets(rew, val, /, gam, lam, *, fin):\n",
    "    r\"\"\"Compute the targets (GAE, returns) for policy grads and critic loss.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    The arguments `rew`, `fin`, and `val` are $r_t$, $d_t$ and $v(s_t)$,\n",
    "    respectively! The td-error terms in GAE depend on $r_{t+1}$, $d_{t+1}$\n",
    "    and on both $v(s_{t+1})$ and $v(s_t)$, hence on `rew[1:]`, `fin[1:]`,\n",
    "    `val[1:]` and `val[:-1]`. `val[-1]` is the value-to-go estimate for\n",
    "    the last state in the related trajectory fragment.\n",
    "    \"\"\"\n",
    "    gae, ret = {}, {}\n",
    "    for k in rew:\n",
    "        ret[k], gae[k] = pyt_ret_gae(\n",
    "            rew[k][1:], fin[1:], val[k],\n",
    "            gam=gam[k], lam=lam[k],\n",
    "        )\n",
    "\n",
    "    return gae, ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbd05b",
   "metadata": {},
   "source": [
    "A function to produce masks for loss dropout, to simulate random batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ae2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import dropout_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aaf7c6",
   "metadata": {},
   "source": [
    "The Advantage (GAE) Actor-Critic loss on a fragment.\n",
    "* this one is on-policy, thus it expects the data in `vp` to be diff-able."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d991a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c_gae(input, vp, *, gam, lam, alpha, mask=None):\n",
    "    \"\"\"do GAE-A2C learning w. intrinsic motivation.\"\"\"\n",
    "    # (gae) compute GAE and returns for all rewards\n",
    "    # XXX r_{t+1}, v_t, v{t+1} -->> A_t, R_t; `rew[t]`, `fin[t]`, and\n",
    "    # `val[t]` must be $r_t$, $d_t$ and $v(s_t)$, respectively, where\n",
    "    # `v_t` is computed based on the historical data $\\cdot_s$ with `s < t`!\n",
    "    gae, ret = pg_targets(input.rew, vp.val, gam, lam, fin=input.fin)\n",
    "\n",
    "    # (gae + motivation) get the weighted sum of advantages\n",
    "    adv = reduce(gae, alpha).detach()\n",
    "\n",
    "    return {\n",
    "        # (sys) policy grad surrogate `sg(G_t) \\log \\pi_t(a_t)`\n",
    "        'polgrad': plyr.apply(pyt_polgrad, vp.pol, input.act, adv=adv, mask=mask),\n",
    "        # (sys) entropy of the policy\n",
    "        'entropy': plyr.apply(pyt_entropy, vp.pol, mask=mask),\n",
    "        # (sys) intrinsic/extrinsic critic loss\n",
    "        'critic': plyr.apply(pyt_critic, vp.val, ret, mask=mask),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf8f33",
   "metadata": {},
   "source": [
    "The PPO + GAE.\n",
    "\n",
    "Recall tha we have\n",
    "$$\n",
    "\\mathbb{E}_{a \\sim \\pi}\n",
    "    f_a \\nabla\\log\\pi_a\n",
    "    = \\mathbb{E}_{a \\sim \\mu}\n",
    "        \\frac{\\pi_a}{\\mu_a}\n",
    "        f_a \\nabla\\log\\pi_a\n",
    "    = \\nabla \\mathbb{E}_{a \\sim \\mu}\n",
    "        \\frac{\\pi_a}{\\mu_a}\n",
    "        \\operatorname{sg}(f_a)\n",
    "    = \\nabla \\mathbb{E}_{a \\sim \\pi}\n",
    "        \\operatorname{sg}(f_a)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_gae(input, vp, myu, *, gam, lam, alpha, eps=0.2, mask=None):\n",
    "    # (gae) compute GAE and returns for all rewards\n",
    "    gae, ret = pg_targets(input.rew, vp.val, gam, lam, fin=input.fin)\n",
    "\n",
    "    # (gae + motivation) get the weighted sum of advantages\n",
    "    adv = reduce(gae, alpha).detach()\n",
    "\n",
    "    # (ppo) get the action likelihood ratio \\rho. It must be\n",
    "    #  diff-able w.r.t. the current policy `vp.pol`.\n",
    "    rho = pyt_logpact(vp.pol, input.act) - pyt_logpact(myu.pol, input.act)\n",
    "\n",
    "    # (ppo) importance weighted PPO loss `\\ell(\\frac{\\pi_t(a_t)}{\\mu_t(a_t)}, A_t)`\n",
    "    # XXX PPO disables grad feedback outside the $1 \\pm \\varepsilon$ trust\n",
    "    #  region of the likelihood, depending on the sign of the advantage\n",
    "    #  estimate, either undoes overly probable disadvantageous actions or\n",
    "    #  reinforces confidence in profitable actions within the region.\n",
    "    lik = rho.exp()\n",
    "    polgrad = torch.minimum(\n",
    "        adv * lik,\n",
    "        adv * lik.clamp(1. - eps, 1. + eps)  # XXX can use .expm1\n",
    "    )\n",
    "\n",
    "    # apply the batching mask\n",
    "    if mask is not None:\n",
    "        scale = float(mask.sum()) / mask.numel()\n",
    "        polgrad = polgrad.mul(mask).div(scale)\n",
    "\n",
    "    return {\n",
    "        'polgrad': polgrad.sum(),\n",
    "        # (sys) entropy of the policy\n",
    "        'entropy': plyr.apply(pyt_entropy, vp.pol, mask=mask),\n",
    "        # (sys) intrinsic/extrinsic critic loss\n",
    "        'critic': plyr.apply(pyt_critic, vp.val, ret, mask=mask),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f66ef",
   "metadata": {},
   "source": [
    "A helper for computing V-trace targets, that takes care of chipping\n",
    "off the initial `rew` and `fin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12674e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.returns import pyt_td_target, pyt_vtrace\n",
    "from nle_toolbox.utils.rl.returns import trailing_broadcast\n",
    "\n",
    "def pyt_vtrace_helper(rew, val, gam, *, fin, rho, r_bar, c_bar):\n",
    "    return pyt_vtrace(rew[1:], fin[1:], val, rho, gam=gam, r_bar=r_bar, c_bar=c_bar)\n",
    "\n",
    "def pyt_td_target_helper(rew, vtrace, val, gam, *, fin, rho):\n",
    "    # [O(T B F)] get the importance-weighted td(0) error advantages\n",
    "    # see sec. \"v-trace actor-critic algo\" (p. 4) in Espeholt et al. (2018)\n",
    "    target = pyt_td_target(rew[1:], fin[1:], vtrace, gam=gam)\n",
    "    return target.sub(val[:-1]).mul(trailing_broadcast(rho, rew))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846bc4b",
   "metadata": {},
   "source": [
    "A procedure to get the likelihood of actions uder a given policy sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09839037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import pyt_logpact\n",
    "\n",
    "def impala(input, vp, myu, *, gam, alpha, r_bar, c_bar, mask=None):\n",
    "    # (impala) For the advantages targets, IWs and critic targets we\n",
    "    #  assume `vp` is diff-able, while `myu` isn't. Both are unstructured.\n",
    "    val_, pol_ = plyr.apply(torch.Tensor.detach, vp)\n",
    "\n",
    "    # (impala) get the importance weights\n",
    "    # XXX `act[t]` is $a_{t-1}$, pol[t] is $\\pi_t$ and $a_t \\sim \\pi_t$\n",
    "    rho = pyt_logpact(pol_, input.act) - pyt_logpact(myu.pol, input.act)\n",
    "\n",
    "    # (impala) get the V-trace targets $v_t$, t=0..N.\n",
    "    # XXX Bad reading of the original paper (Espeholt et al.; 2018)) could\n",
    "    #  wrongfully suggest that IMPALA uses values form the behavioural policy\n",
    "    #  `myu`, rather than `vp` from the current policy.\n",
    "    #  The V-trace targets, defined by eq. (1) in sec. 4.1 (p. 3, ibid), are\n",
    "    #  in fact the result of applying the V-trace operator R (app. A.1, ibid)\n",
    "    #  to SOME state-value function V. Furthermore, the paper never specifies\n",
    "    #  whether the value estimates are bundled with the policy. Given this\n",
    "    #  and the simultaneous on/off-policy use of V-trace, it should be understood\n",
    "    #  that the section `V-trace A-C` (sec 4.2, ibid) uses the value estimates\n",
    "    #  related to the current policy. Third-party implementations, e.g. RIDE's\n",
    "    #  codebase, corroborate this.\n",
    "    # XXX `rew[t], fin[t]` ($r_t, f_t$) PRECURSE `val[t]` ($v_t$), hence\n",
    "    # we drop the first values from `rew` and `fin`.\n",
    "    vtrace = plyr.apply(pyt_vtrace_helper, input.rew, val_, gam,\n",
    "                        fin=input.fin, rho=rho, r_bar=r_bar, c_bar=c_bar)\n",
    "\n",
    "    # (impala) get the importance-weighted td(0)-error advantages.\n",
    "    # XXX unclear if we backprop thru `vp.val` here if the `val`\n",
    "    # and `pol` networks have shared parameters.\n",
    "    adv = plyr.apply(\n",
    "        pyt_td_target_helper, input.rew, vtrace, val_, gam,\n",
    "        fin=input.fin, rho=rho.exp().clamp_(max=r_bar))\n",
    "\n",
    "    # (motivation) get the weighted sum of advantages\n",
    "    adv = reduce(adv, alpha).detach()\n",
    "    vtarget = plyr.apply(lambda t: t[:-1], vtrace)\n",
    "\n",
    "    return {\n",
    "        # (sys) policy grad surrogate `sg(A_t) \\log \\pi_t(a_t)`\n",
    "        'polgrad': pyt_polgrad(vp.pol, input.act, adv=adv, mask=mask),\n",
    "        # (sys) entropy of the policy\n",
    "        'entropy': plyr.apply(pyt_entropy, vp.pol, mask=mask),\n",
    "        # (sys) intrinsic/extrinsic critic loss\n",
    "        'critic': plyr.apply(pyt_critic, vp.val, vtarget, mask=mask),\n",
    "    }, rho.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1461a",
   "metadata": {},
   "source": [
    "Rewarding impact-driven exploration\n",
    "$$\n",
    "\\begin{equation}\n",
    "    x_t, x_{t+1}\n",
    "        \\overset{\\mathrm{RIDE}}{\\longrightarrow}\n",
    "            {\\color{orange}{r^I_{t+1}}}\n",
    "            = \\bigl\\|\\phi(x_{t+1}) - \\phi(x_t)\\bigr\\|\n",
    "        \\,.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_\\mathrm{inv}\n",
    "    = - \\frac1{T-1} \\sum_{t=1}^{T-1} \\log p(a_{t-1} \\mid x_t, x_{t-1})\n",
    "  \\,, $$\n",
    "with\n",
    "$$\n",
    "p(a_{t-1} \\mid x_t, x_{t-1})\n",
    "    \\propto f(\\phi(x_t), \\phi(x_{t-1}))\n",
    "    \\,. $$\n",
    "\n",
    "$$\n",
    "L_\\mathrm{fwd}\n",
    "    = - \\frac1{T-1} \\sum_{t=1}^{T-1}\n",
    "        \\log p(x_t \\mid x_{t-1}, a_{t-1})\n",
    "    \\,, $$\n",
    "with\n",
    "$$\n",
    "p(x_t \\mid x_{t-1}, a_{t-1})\n",
    "    \\propto g(\\phi(x_{t-1}), a_{t-1})\n",
    "    \\,. $$\n",
    "* $x_t$ -- the current observation (we allow partial observability)\n",
    "* $a_t$ -- the action to be taken in the `env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a947be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ride(module, emb0, emb1, act, *, b_detach=False, mask=None):\n",
    "    # (ride) predict \\xi_{t-1}, \\xi_t -->> \\pi(a_{t-1}) for t=1..T with\n",
    "    #  the cross-entropy loss.\n",
    "    # XXX we backprop into $\\phi$ thru BOTH $\\xi_{t-1}$ and $\\xi_t$,\n",
    "    #  `emb0[t]` and `emb1[t]`, respectively.\n",
    "    out_inv = module.inv((emb0, emb1))\n",
    "    loss_inv = F.cross_entropy(\n",
    "        out_inv.flatten(0, 1), act.flatten(0, 1), reduction='none'\n",
    "    ).reshape_as(act)\n",
    "\n",
    "    # (ride) regress \\xi_{t-1}, a_{t-1} -->> \\xi_t for t=1..T w. ell-2 loss\n",
    "    # XXX we could treat $\\xi_t$ as a fixed target, and NOT backprop thru it!\n",
    "    out_fwd = module.fwd((emb0, module.act(act)))\n",
    "    loss_fwd = F.mse_loss(\n",
    "        out_fwd, (emb1.detach() if b_detach else emb1), reduction='none',\n",
    "    ).sum(-1)  # reduce over the embedding dim (output vector)\n",
    "\n",
    "    # (sys) skip the first embedding (t=0) since it has been processed in\n",
    "    #  the previous fragment. Besides, the agent has already acted upon the\n",
    "    #  data in `input[T]` (verify).\n",
    "    loss_inv = loss_inv[1:]\n",
    "    loss_fwd = loss_fwd[1:]\n",
    "    if mask is not None:\n",
    "        scale = float(mask.sum()) / mask.numel()\n",
    "        loss_inv = loss_inv.mul(mask).div(scale)\n",
    "        loss_fwd = loss_fwd.mul(mask).div(scale)\n",
    "\n",
    "    return {'fwd': loss_fwd.sum(), 'inv': loss_inv.sum()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e0e50",
   "metadata": {},
   "source": [
    "Progress bar update and termination condition checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def progress(bar, n):\n",
    "    bar.update(n - bar.n)\n",
    "    return n < bar.total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00e4ff",
   "metadata": {},
   "source": [
    "Decorators for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def capture(fn, to):\n",
    "    \"\"\"Capture the log information output to the specified dict.\n",
    "    \"\"\"\n",
    "    if to is None:\n",
    "        return fn\n",
    "\n",
    "    @wraps(fn)\n",
    "    def _wrapper(*args, **kwargs):\n",
    "        nfo, _ = result = fn(*args, **kwargs)\n",
    "        to.update(nfo)\n",
    "        return result\n",
    "\n",
    "    return _wrapper\n",
    "\n",
    "def sniff(fn, to):\n",
    "    \"\"\"Record inputs and outputs to the specified list.\n",
    "    \"\"\"\n",
    "    if to is None:\n",
    "        return fn\n",
    "\n",
    "    @wraps(fn)\n",
    "    def _wrapper(*args, **kwargs):\n",
    "        to.append((args, kwargs))\n",
    "        return fn(*args, **kwargs)\n",
    "\n",
    "    return _wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc338825",
   "metadata": {},
   "source": [
    "A service function to get diagnostic stats and exploration metrics from an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c335d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nle.nethack import NLE_BL_SCORE\n",
    "from nle_toolbox.utils.env.defs import MAX_ENTITY, GLYPH_CMAP_OFF, symbol\n",
    "from nle.env.base import NLE\n",
    "\n",
    "from collections import namedtuple\n",
    "Episode = namedtuple('Episode', 'input,output,info')\n",
    "\n",
    "def ep_stats(ep, *, S_stone=symbol.S_stone + GLYPH_CMAP_OFF):\n",
    "    assert isinstance(ep, Episode)\n",
    "\n",
    "    met = {}\n",
    "    # determine the offset for unfinished episodes\n",
    "    offset = int(ep.input.fin[-1])\n",
    "    if len(ep.input.fin) <= offset:\n",
    "        return met\n",
    "\n",
    "    # convert to numpy `npy = plyr.apply(np.asarray, ep)`\n",
    "    # episode duration and total score\n",
    "    n_length = len(ep.input.fin) - offset\n",
    "\n",
    "    # score the episode (return = \\sum_j r_{j+1} = sum r[j], j=1..N-1)\n",
    "    f_return = plyr.apply(lambda r: r[1:].sum(), ep.input.rew)\n",
    "    f_score = ep.input.obs['blstats'][-1-offset, NLE_BL_SCORE]\n",
    "\n",
    "    # count the number of unique glyphs seen during the episode\n",
    "    vic = ep.input.obs['vicinity'][:(-1 if offset > 0 else None)]\n",
    "    cnt = torch.bincount(vic.flatten(), minlength=MAX_ENTITY + 1)\n",
    "    n_unique = cnt.gt(0).sum()\n",
    "\n",
    "    # The empirical entropy is a fine proxy for the diversity, since\n",
    "    # it measures the amount of information content the seen glyphs.\n",
    "    proba = cnt.div(cnt.sum())\n",
    "    f_entropy = F.kl_div(proba.new_zeros(()), proba, reduction='sum').neg()\n",
    "\n",
    "    # coverage and action effectiveness\n",
    "    gly = ep.input.obs['glyphs'][:(-1 if offset > 0 else None)]\n",
    "    # XXX we exclude the terminal obs, because it is actually the init\n",
    "    #  obs from the next episode\n",
    "    non_stone = (gly != S_stone).float().mean((-2, -1))\n",
    "    f_cov = non_stone.max() / non_stone.min()\n",
    "    f_eff = sum((g0 != g1).sum() / g1.numel() for g0, g1 in zip(gly, gly[1:]))\n",
    "    \n",
    "    # inspect the last info dict of the episode\n",
    "    b_death = np.nan\n",
    "    if ep.info:\n",
    "        nfo = plyr.apply(lambda x: x[-1].item(), ep.info)\n",
    "\n",
    "        # indicate if the episode ended in agent's death\n",
    "        b_death = float(nfo['end_status'] == NLE.StepStatus.DEATH)\n",
    "\n",
    "    return {\n",
    "        'duration': n_length,\n",
    "        'return': plyr.apply(float, f_return),\n",
    "        'score': int(f_score),\n",
    "        'n_unique': int(n_unique),\n",
    "        'diversity': float(f_entropy) / math.log(2),\n",
    "        'coverage': float(f_cov),\n",
    "        'effectiveness': float(f_eff),\n",
    "        'b_death': b_death,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd7e26",
   "metadata": {},
   "source": [
    "Aggregate the diagnostic stats across several episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ep_aggregate(episodes):\n",
    "    # filter out length one episodes\n",
    "    metrics = list(filter(bool, map(ep_stats, episodes)))\n",
    "    if not metrics:\n",
    "        return {}\n",
    "\n",
    "    # share of episodes that ended in agent's death\n",
    "    f_deaths = np.mean([m['b_death'] for m in metrics])\n",
    "    metrics = plyr.apply(np.median, *metrics, _star=False)\n",
    "\n",
    "    metrics['b_death'] = f_deaths\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8edbf0a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d43ff",
   "metadata": {},
   "source": [
    "Create the vectorized env and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0623ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from nle_toolbox.utils.rl.engine import SerialVecEnv\n",
    "\n",
    "config = deepcopy(dict(wandb.config))\n",
    "env = SerialVecEnv(\n",
    "    factory,\n",
    "    n_envs=wandb.config.n_batch,\n",
    "    kwargs=dict(config=config),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b6608",
   "metadata": {},
   "source": [
    "How to:\n",
    "* organize the intrinsic motivator module?\n",
    "  * ims are like regular actors, except their `actions` are rewards!\n",
    "```python\n",
    "    class SelfSupervisedRewards(nn.Module):\n",
    "        def forward(self, obs, act=None, rew=None, fin=None, *, hx=None):\n",
    "            return rew, (), hx\n",
    "```\n",
    "  * let's make intrinsict motivation modules be truly __exonegeous__ wrt the agents\n",
    "* set up the optimizers for various submodules?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401647fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class BaseLearner(nn.Module):\n",
    "    def __init__(self, agent, *, cfg: dict):\n",
    "        super().__init__()\n",
    "        self.agent = agent\n",
    "        self.cfg = cfg\n",
    "        self.epx = EpisodeExtractor()\n",
    "\n",
    "    def forward(self, input, *, hx=None):\n",
    "        return self.agent(**input._asdict(), hx=hx)\n",
    "\n",
    "    def learn(self, input, vp=None, *, gx=None, hx=None, nfo=None):\n",
    "        cfg = self.cfg\n",
    "        if vp is None:\n",
    "            _, vp, _ = self(input, hx=gx)\n",
    "\n",
    "        # (sys) do GAE-A2C learning w. intrinsic motivation `vp` is (v_t, \\pi_t)\n",
    "        # ATTN the first pass is full batch?\n",
    "        terms = a2c_gae(\n",
    "            input,\n",
    "            vp,\n",
    "            gam=cfg['f_gamma'],\n",
    "            lam=cfg['f_lambda'],\n",
    "            alpha=cfg['f_alpha'],\n",
    "            mask=None,\n",
    "        )\n",
    "        loss = reduce(terms, cfg['f_coefs'])\n",
    "\n",
    "        # (sys) extra batch steps thru the current fragment with IMPALA\n",
    "        myu = plyr.apply(torch.Tensor.detach, vp)\n",
    "        for _ in range(cfg['n_off_policy']):\n",
    "            # (sys) backprop through the agent\n",
    "            self.agent.optim.zero_grad(True)\n",
    "            loss.backward()\n",
    "            grad = clip_grad_norm_(self.parameters(), cfg['f_grad_norm'])\n",
    "            self.agent.optim.step()\n",
    "\n",
    "            # (batching) bet the batch mask for the current fragment\n",
    "            mask = dropout_mask(input, k=cfg['n_batch_size'])\n",
    "\n",
    "            # (off-policy) additional pass thru the fragment\n",
    "            _, vp, _ = self(input, hx=gx)\n",
    "            if cfg['s_off_alg'] == 'impala':\n",
    "                terms, _ = impala(input, vp, myu, gam=cfg['f_gamma'],\n",
    "                                  alpha=cfg['f_alpha'], r_bar=1., c_bar=1.,\n",
    "                                  mask=mask)\n",
    "\n",
    "            elif cfg['s_off_alg'] == 'ppo':\n",
    "                terms = ppo_gae(input, vp, myu, gam=cfg['f_gamma'],\n",
    "                                lam=cfg['f_lambda'], alpha=cfg['f_alpha'],\n",
    "                                eps=0.2, mask=mask)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown algorithm `{cfg['s_off_alg']}`.\")\n",
    "\n",
    "            loss = reduce(terms, cfg['f_coefs'])\n",
    "\n",
    "        # (sys) backprop through the agent\n",
    "        self.agent.optim.zero_grad(True)\n",
    "        loss.backward()\n",
    "        grad = clip_grad_norm_(self.parameters(), cfg['f_grad_norm'])\n",
    "        self.agent.optim.step()\n",
    "\n",
    "        # (sys) extract episode strands (drop the last record, due to overlap)\n",
    "        # XXX force a clone, since `input` might be overwritten, and epx slices!\n",
    "        input = plyr.apply(lambda x: x[:-1].clone(), input)\n",
    "        episodes = self.epx.extract(input.fin, Episode(\n",
    "            input,\n",
    "            (),\n",
    "            plyr.apply(torch.as_tensor, *nfo, _star=False),\n",
    "        ))\n",
    "\n",
    "        # (sys) recompute the recurrent state `hx` AFTER the update over the\n",
    "        # proper part of the fragment (t=0..N-1)\n",
    "        if hx is not None:\n",
    "            # `hx` is $h_N$ from $h_{t+1}, y_t = F(z_t, h_t; w)$, t=0..N-1.\n",
    "            # One of update's side effects is that the recurrent runtime state\n",
    "            #  in `hx` became stale, i.e. it no longer corresponds to the final\n",
    "            #  state had the updated policy been run on the same fragment the\n",
    "            #  second time. We would recompute `hx` over the entire historical\n",
    "            #  trajectory, had we stored it whole. The next best option is to\n",
    "            #  make a second pass over the just collected fragment $\n",
    "            #      (z_t)_{t=0}^{N-1}\n",
    "            #  $ and use $h'_N$ as the new `hx`, where $\n",
    "            #      h'_{t+1}, y_t = F(z_t, h'_t; w')\n",
    "            #  $ with $h'_0 = h_0$ given by `gx`.\n",
    "            with torch.no_grad():\n",
    "                _, _, hx = self(input, hx=gx)\n",
    "\n",
    "            # DO NOT backprop through `hx` from the next fragment (truncated bptt!\n",
    "            hx = plyr.apply(torch.Tensor.detach, hx)\n",
    "            \n",
    "            # pass some grad feedback from the next fragment to `h0`\n",
    "            h0 = self.agent.initial_hx\n",
    "            f_h0_lerp = cfg['f_h0_lerp']\n",
    "            if h0 is not None and f_h0_lerp > 0:\n",
    "                hx = plyr.apply(torch.lerp, hx, h0, weight=f_h0_lerp)\n",
    "\n",
    "        # the batch-size normalization (TxB) is carried out on the logger's side\n",
    "        terms = plyr.apply(float, terms)\n",
    "        out = {'loss/loss': float(loss), 'loss/grad': float(grad)}\n",
    "        out.update({'loss/' + k: v for k, v in terms.items()})\n",
    "\n",
    "        # report diagnostic stats for completed episodes\n",
    "        out.update({'metrics/' + k: v for k, v in ep_aggregate(episodes).items()})\n",
    "\n",
    "        return out, hx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eec5c1",
   "metadata": {},
   "source": [
    "RND motivator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c233c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class RNDMotivator(nn.Module):\n",
    "    def __init__(self, rnd, *, cfg: dict):\n",
    "        super().__init__()\n",
    "        self.rnd = rnd\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, input, *, hx=None):\n",
    "        \"\"\"Reward observational novelty based on Random Network Distillation.\n",
    "        \"\"\"\n",
    "        # get the non-diffable intrinsic rewards\n",
    "        rew, out, hx = self.rnd(**input._asdict(), hx=hx)\n",
    "        \n",
    "        # clamp it, and return the rewards and outputs\n",
    "        f_rew_max = self.cfg['f_rew_max']\n",
    "        if f_rew_max is not None:\n",
    "            rew.clamp_(max=f_rew_max)\n",
    "        return rew, out, hx\n",
    "\n",
    "    def learn(self, input, error=None, *, gx=None, hx=None, nfo=None):\n",
    "        terms, grad = {}, float('nan')\n",
    "        for _ in range(self.cfg['n_batches']):\n",
    "            # We've got the following `input`: `.act[t]` is $r^I_{t-1}$\n",
    "            #  `.obs.agent[t]` is $a_{t-1}$ taken in the env to get $x_t$,\n",
    "            #  which is stroed in `.obs.obs[t]`.\n",
    "            if error is None:\n",
    "                _, error, _ = self(input, hx=gx)\n",
    "\n",
    "            # (sys) skip the first item since it has been processed in a\n",
    "            # previous fragment\n",
    "            error = error[1:]\n",
    "\n",
    "            # (batching) bet the batch mask for the current fragment\n",
    "            mask = dropout_mask(input, k=self.cfg['n_batch_size'])\n",
    "            if mask is not None:\n",
    "                scale = float(mask.sum()) / mask.numel()\n",
    "                error = error.mul(mask).div(scale)\n",
    "\n",
    "            loss = error.sum()\n",
    "            self.rnd.optim.zero_grad(True)\n",
    "            loss.backward()\n",
    "            grad = clip_grad_norm_(self.parameters(), self.cfg['f_grad_norm'])\n",
    "            self.rnd.optim.step()\n",
    "\n",
    "            # invalidate the errors\n",
    "            error = None\n",
    "\n",
    "        return {\n",
    "            'loss/rnd': float(loss),\n",
    "            'loss/mot.grad': float(grad),\n",
    "        }, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d54171",
   "metadata": {},
   "source": [
    "Intrinsic motivation using RIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIDEMotivator(nn.Module):\n",
    "    def __init__(self, ride, *, cfg: dict):\n",
    "        super().__init__()\n",
    "        self.ride = ride\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, input, *, hx=None):\n",
    "        \"\"\"Impact-driven rewards.\n",
    "        \"\"\"\n",
    "        # get the non-diffable intrinsic rewards\n",
    "        rew, emb, hx = self.ride(**input._asdict(), hx=hx)\n",
    "\n",
    "        # clamp it, and return the rewards and embeddings\n",
    "        f_rew_max = self.cfg['f_rew_max']\n",
    "        if f_rew_max is not None:\n",
    "            rew.clamp_(max=f_rew_max)\n",
    "        return rew, emb, hx\n",
    "\n",
    "    def learn(self, input, emb=None, *, gx=None, hx=None, nfo=None):\n",
    "        assert isinstance(input.obs, MotivatorObs)\n",
    "        terms, grad = {}, float('nan')\n",
    "        for _ in range(self.cfg['n_batches']):\n",
    "            # input.obs.agent are actions ALREADY taken by the agent in the env!\n",
    "            #  the last one is the one for which the capsule will compute the reward\n",
    "            #  right after the call to `.learn` (all rewards received, except for\n",
    "            #  the last one in `rew_`, the received rewards are in `input.act`)\n",
    "            if emb is None:\n",
    "                _, emb, _ = self(input, hx=gx)\n",
    "\n",
    "            # (batching) bet the batch mask for the current fragment\n",
    "            mask = dropout_mask(input, k=self.cfg['n_batch_size'])\n",
    "\n",
    "            # get the actions of the agent and the embeddings\n",
    "            # XXX `.agent[t]` is $a_{t-1}$, while `emb[t]` is\n",
    "            #     $(z_{t-1}, z_t)$, $z_t = \\phi(x_t)$, t=0..T\n",
    "            terms = ride(self.ride, *emb, input.obs.agent,\n",
    "                         b_detach=self.cfg['b_detach'], mask=mask)\n",
    "            loss = reduce(terms, self.cfg['f_coefs'])\n",
    "\n",
    "            self.ride.optim.zero_grad(True)\n",
    "            loss.backward()\n",
    "            grad = clip_grad_norm_(self.parameters(), self.cfg['f_grad_norm'])\n",
    "            self.ride.optim.step()\n",
    "\n",
    "            # invalidate the embeddings\n",
    "            emb = None\n",
    "\n",
    "        # (ride) recompute `hx` so that the initial embedding is correct,\n",
    "        #  however do not waste compute on the whole fragment: since RIDE\n",
    "        #  itself is non-recurrent and just applies embedder to the observations\n",
    "        #  independently, we may just use the last observation in the proper\n",
    "        #  fragment.\n",
    "        # XXX clone for diffability, because `input` might be updated in-place\n",
    "        with torch.no_grad():\n",
    "            _, _, hx = self(plyr.apply(lambda x: x[-2:-1].clone(), input), hx=None)\n",
    "\n",
    "            # DO NOT backprop through `hx` from the next fragment (truncated bptt!\n",
    "            hx = plyr.apply(torch.Tensor.detach, hx)\n",
    "\n",
    "        return {\n",
    "            'loss/ride': plyr.apply(float, terms),\n",
    "            'loss/mot.grad': float(grad),\n",
    "        }, hx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e3802",
   "metadata": {},
   "source": [
    "problem:\n",
    "* motivator need access to $a_t$ -- the action, which caused the transition $x_t \\to x_{t+1}$ in the env. Therefore we cannot simply change the action space of the motivator to the rewards, unless we augmetn\n",
    "its observation space by the action during init."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc24591",
   "metadata": {},
   "source": [
    "Create the learner and motivator instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = BaseLearner(agent, cfg=wandb.config.act)\n",
    "\n",
    "motivator = None\n",
    "if mot is not None:\n",
    "    Motivator = {\n",
    "        str(RNDModule): RNDMotivator,\n",
    "        str(RIDEModule): RIDEMotivator,\n",
    "    }[cls_motivator]\n",
    "    motivator = Motivator(mot, cfg=wandb.config.mot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578ccdb",
   "metadata": {},
   "source": [
    "At time $t \\geq 0$ we do the following for each $\n",
    "    j=0, 1, .., J-1\n",
    "$ in sequence: at $t \\to t+1$\n",
    "$$\n",
    "\\omega^j_t, \\bigl(\n",
    "    u^{< j}_{t+1},\n",
    "    u^{\\geq j}_t\n",
    "\\bigr), \\bigl(\n",
    "    r^{< j}_{t+1},  % can see all rewards for historical observables\n",
    "    r^{\\geq j}_t\n",
    "\\bigr)\n",
    "    \\overset{L_j}{\\longrightarrow}\n",
    "        \\omega^j_{t+1},\n",
    "        u^j_{t+1},\n",
    "        r^j_{t+1}\n",
    "    \\,, $$\n",
    "where $u^{< j}_{t+1}$ are __observables__ issued by blocks prior to $j$\n",
    "during their $t \\to t+1$ transition, $u^{\\geq j}_t$ --- the historical\n",
    "time $t$ observables of all blocks after and including the $j$-th.\n",
    "\n",
    "An __observable__ $u^j_{t+1}$ is anything that block $j$ may generate:\n",
    "an action $a_{t+1}$ to be fed into a subsequent block, a partial observation\n",
    "$x_{t+1}$, which can be used by the next block, or a reward $r_{t+1}$, which\n",
    "the next block reveives for it historical __observable__.\n",
    "<!--  -->\n",
    "Despite being observables, we keep the rewards separate from $u^\\cdot_t$, because\n",
    "of their special role in the RL objective. The reward $r^{j-1}_{t+1}$ is issued\n",
    "during $t \\to t+1$ by the block $j-1$ for the historical observable $u^j_t$ produced\n",
    "by block $j$ at $t-1 \\to t$ transition.\n",
    "\n",
    "The joint system is initialized at $t=-1$ with special values (default or random)\n",
    "for $u^j_{-1}, r^j_{-1}$ and $\\omega^j_{-1}$, that signify initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d128bca",
   "metadata": {},
   "source": [
    "For example, a classical binary env-agent interaction system has the following stepping:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\underbrace{\\omega_t}_{\\omega^0_t}, \\bigl(\n",
    "        \\emptyset,\n",
    "        \\bigl(\n",
    "            \\underbrace{\\emptyset}_{u^0_t},\n",
    "            \\underbrace{a_t}_{u^1_t}\n",
    "        \\bigr)\n",
    "    \\bigr), \\bigl(\n",
    "        \\emptyset,\n",
    "        \\bigl(\n",
    "            \\underbrace{\\emptyset}_{r^0_t},\n",
    "            \\underbrace{\\emptyset}_{r^1_t}\n",
    "        \\bigr)\n",
    "    \\bigr)\n",
    "        &\\overset{\\text{ENV}}{\\longrightarrow}\n",
    "            \\underbrace{\\omega_{t+1}}_{\\omega^0_{t+1}},\n",
    "            \\underbrace{x_{t+1}}_{u^0_{t+1}},\n",
    "            \\underbrace{r_{t+1}}_{r^0_{t+1}}\n",
    "        \\,, \\\\\n",
    "    \\underbrace{h_t}_{\\omega^1_t}, \\bigl(\n",
    "        \\underbrace{x_{t+1}}_{u^0_{t+1}},\n",
    "        \\underbrace{a_t}_{u^1_t}\n",
    "    \\bigr), \\bigl(\n",
    "        \\underbrace{r_{t+1}}_{r^0_{t+1}},\n",
    "        \\underbrace{\\emptyset}_{r^1_t}\n",
    "    \\bigr)\n",
    "        &\\overset{\\text{ACT}}{\\longrightarrow}\n",
    "            \\underbrace{h_{t+1}}_{\\omega^1_{t+1}},\n",
    "            \\underbrace{a_{t+1}}_{u^0_{t+1}},\n",
    "            \\underbrace{\\emptyset}_{r^1_{t+1}}\n",
    "        \\,,\n",
    "\\end{align}\n",
    "$$\n",
    "since **env** does not use its own observables it emitted in the past ($u^0_t$),\n",
    "and reacts only to the observables produced by the actor, which are, in this\n",
    "case, *actions* ($u^1_t = a_t$). Similarly, the **env** completely ignores all\n",
    "rewards ($r^j_t$), unlike the actor **act**, which uses the env's reward to\n",
    "reinfoce its historical actions ($r^0_{t+1}$ for the action $a_t$). As for\n",
    "the observables, the actor makes use of the recent and heistorical observables\n",
    "emitted by *env* ($u^0_s$ for $s \\leq t+1$) and its own historical action\n",
    "($u^1_t = a_t$) in order to produce the next reactive action ($u^1_{t+1} = a_{t+1}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfceb44",
   "metadata": {},
   "source": [
    "learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84479779",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import Input, prepare\n",
    "from nle_toolbox.utils.rl.capsule import capsule, buffered, launch\n",
    "from nle_toolbox.zoo.models.motivation import MotivatorObs\n",
    "\n",
    "log = {}\n",
    "data_agnt = None\n",
    "data_motv = None\n",
    "\n",
    "caps = buffered(\n",
    "    learner,\n",
    "    capture(sniff(learner.learn, data_agnt), log),\n",
    "    length=wandb.config.act['n_length'],\n",
    "    device=device_\n",
    ")\n",
    "\n",
    "if motivator is not None:\n",
    "    motv = buffered(\n",
    "        motivator,\n",
    "        capture(sniff(motivator.learn, data_motv), log),\n",
    "        length=wandb.config.mot['n_length'],\n",
    "        device=device_,\n",
    "    )\n",
    "\n",
    "n_steps = 0\n",
    "with tqdm.tqdm(\n",
    "    initial=n_steps,\n",
    "    total=wandb.config.n_total,\n",
    "    ncols=70,\n",
    "    disable=False,\n",
    ") as bar:\n",
    "    # the local runtime state for the main exec flow\n",
    "    # env is reset, and act is sampled from the env\n",
    "    #   \\emptyset -->> \\omega_0, x_0, r^E_0, f_0\n",
    "    npy, pyt = prepare(env, rew=0., fin=True)\n",
    "\n",
    "    # specify the motivator and get its first reward action\n",
    "    #   \\xi_0, ((x_0, a_{-1}), g_{-1}, r^E_0, f_0) -->> \\xi_1, g_0\n",
    "    npy_mot_rew = np.zeros_like(npy.rew)\n",
    "    if motivator is not None:\n",
    "        obs = MotivatorObs(npy.act, npy.obs)\n",
    "        act = np.full_like(npy_mot_rew, np.nan)\n",
    "        npy_mot_rew = launch(motv, Input(obs, act, npy.rew, npy.fin))\n",
    "\n",
    "    # recv the first action of the encapsulated agent\n",
    "    #   h_0, (x_0, a_{-1}, (r^E_0, g_0), f_0) -->> h_1, a_0\n",
    "    rew = {'ext': npy.rew, 'int': npy_mot_rew}\n",
    "    act = launch(caps, Input(npy.obs, npy.act, rew, npy.fin))\n",
    "\n",
    "    # step thru the environment\n",
    "    #   \\omega_0, a_0 -->> \\omega_1, x_1, r^E_1, f_1\n",
    "    obs, rew, fin, nfo = env.step(act)\n",
    "    n_steps += len(env)\n",
    "\n",
    "    # the main loop\n",
    "    while progress(bar, n_steps):\n",
    "        # compute intrinsic motivation (self-supervised rewards)\n",
    "        #   \\xi_{t-1}, a_{t-1}, x_{t-1}, x_t -->> \\xi_t, r^I_t\n",
    "        if motivator is not None:\n",
    "            npy_mot_rew = motv.send((MotivatorObs(act, obs), rew, fin, nfo))\n",
    "\n",
    "        # issue the rewards r^E_t and r^I_t for the `t-1 -->> t` transition\n",
    "        rew = {'ext': rew, 'int': npy_mot_rew}\n",
    "\n",
    "        # decide which capsule to route the data to and get the next action\n",
    "        #   h_{t-1}, x_t -->> h_t, a_t\n",
    "        act = caps.send((obs, rew, fin, nfo))\n",
    "\n",
    "        # (log) the training progress\n",
    "        if log:\n",
    "            wandb.log({'n_steps': n_steps, **log}, commit=True)\n",
    "            log.clear()\n",
    "\n",
    "        # step thru the environment\n",
    "        #   \\omega_{t-1}, a_{t-1} -->> \\omega_t, x_t, r^E_t, f_t\n",
    "        obs, rew, fin, nfo = env.step(act)\n",
    "        # plyr.apply(np.copyto, npy, Input(obs, act, rew, fin))\n",
    "        n_steps += len(env)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d1824b6",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()  # obs['tty_chars'].view('S80')[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c957f72",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nle_toolbox.utils.io import mkstemp\n",
    "\n",
    "target = os.path.abspath('./checkpoints')\n",
    "os.makedirs(target, exist_ok=True)\n",
    "\n",
    "checkpoint = mkstemp('.pt', f'ckpt__{wandb.run.name}__{n_steps}__', dir=target)\n",
    "torch.save({\n",
    "    'cls': str(NLEAgent),\n",
    "    'agent': agent.state_dict(),\n",
    "    'agent.optim': agent.optim.state_dict(),\n",
    "    'mot': mot.state_dict() if mot is not None else {},\n",
    "    'mot.optim': mot.optim.state_dict() if mot is not None else {},\n",
    "    'config': dict(wandb.config),\n",
    "}, checkpoint)\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6da432",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54264b7d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02820413",
   "metadata": {},
   "source": [
    "We finally got something:\n",
    "* reducing the dim of the RND output vector from 64 to 16 was, perhaps,\n",
    "the most significant step in the direction of the agent learning at least\n",
    "something.\n",
    "* the next thing was removing the build embedding and using `condition`,\n",
    "`hunger`, and `vitals\n",
    "* the effect of positive `f_h0_lerp` has not been investigated\n",
    "  * reducing it from `.05` to `0.0` seems to **adversely** impact learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100e217",
   "metadata": {},
   "source": [
    "Next idea to try: like token + position in BERT and GPT, let's exploit additive embeddings.\n",
    "* in glyphs: entity + group embedding -- entities semantics is modulated by its group (relevant to MON, PET, RIDE, STATUE), also add special `ego`-embedding at the centre of vicinity.\n",
    "Let $g_{uv} \\in \\mathbb{G}$ be glyph at position $u, v$ relative to the `hero` (bls-x, -y coords).\n",
    "$$\n",
    "f_{uv}\n",
    "    = w_E\\bigl[\\operatorname{entity}(g_{uv})\\bigr]\n",
    "    + w_G\\bigl[\\operatorname{group}(g_{uv})\\bigr]\n",
    "    + 1_{(0,0)}{(u, v)} w_{\\mathrm{ego}}\n",
    "  \\,, $$\n",
    "where $w_\\cdot$ are game map-related embeddings\n",
    "* in blstats: modulate an `ego`-embedding by the `vitals`, `condition` and other stats' embeddings.\n",
    "  * should the the `ego`-embedding be shared between map and state?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95345e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce1f33",
   "metadata": {},
   "source": [
    "A ranked buffer for episode rollouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0fc1de",
   "metadata": {},
   "source": [
    "* what do we do with the missing `hx`? clone episodes in full?\n",
    "  * `you wake up on a cold stone floor in the middle of a vast chamber without a shred of memoery of how you got here. What do you do?` maybe it is OK to take contiguous fragments of a long episode and start with a wiped out memory.\n",
    "* do we clone just the actions, or also the value function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heapreplace, heappushpop, heappush, heappop\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "class RankedBuffer:\n",
    "    @dataclass(order=True, frozen=True, repr=False)\n",
    "    class RankedItem:\n",
    "        rank: float\n",
    "        item: Any = field(compare=False)\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, rk, it):\n",
    "        item = self.RankedItem(rk, it)\n",
    "        # push the current item\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            return heappush(self.buffer, item)\n",
    "        # ... pop the lowest-ranking one, if we exceed capacity\n",
    "        return heappushpop(self.buffer, item)\n",
    "\n",
    "    def extend(self, pairs):\n",
    "        last = None\n",
    "        for rk, it in pairs:\n",
    "            last = self.push(rk, it)\n",
    "        return last\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.buffer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.buffer[index]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ + f\"({len(self.buffer)}/{self.capacity})\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        return ((el.rank, el.item) for el in self.buffer)\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        n_samples=8,\n",
    "        n_steps=64,\n",
    "        *,\n",
    "        rng=np.random.default_rng(),\n",
    "    ):\n",
    "        # determinie the sufficiently long episodes\n",
    "        eligible = []\n",
    "        for j, ep in enumerate(self.buffer):\n",
    "            input = ep.item.input\n",
    "            dur_ = len(input.fin) - int(input.fin[-1])\n",
    "            if dur_ < n_steps:\n",
    "                continue\n",
    "\n",
    "            eligible.append((j, dur_,))\n",
    "\n",
    "        # sample starting strands from episodes\n",
    "        chunks = []\n",
    "        for i in rng.choice(len(eligible), size=n_samples):\n",
    "            k, dur_ = eligible[i]\n",
    "            j = rng.integers(dur_ - n_steps + 1)\n",
    "\n",
    "            chunk = plyr.apply(lambda t: t[j:j + n_steps],\n",
    "                               self.buffer[k].item)\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        return plyr.apply(torch.stack, *chunks, _star=False, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ad74d",
   "metadata": {},
   "source": [
    "A common routine to plot the computed rollout metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912caf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ep_metrics(metrics):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(7, 3), dpi=300)\n",
    "\n",
    "    out_ = plyr.apply(list, *metrics, _star=False)\n",
    "    for ax, (nom, val) in zip(axes.flat, out_.items()):\n",
    "        ax.hist(val, label=nom, log=nom in ('score', 'return',), bins=20)\n",
    "        ax.set_title(nom)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581980a",
   "metadata": {},
   "source": [
    "Rank the peisode and put it into a buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_episodes(buf, iterable, *, C_cov=0.1):\n",
    "    for ep in iterable:\n",
    "        met = ep_stats(ep)\n",
    "        if not met:\n",
    "            continue\n",
    "\n",
    "        rk = met['return'] + C_cov * met['coverage'] / met['duration']\n",
    "        buf.push(rk, ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10429b45",
   "metadata": {},
   "source": [
    "One step in the joint differentiable rollout collection and a procedure to collect a differentiable rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41626ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import step\n",
    "\n",
    "def collect(env, agent, npyt, hx, *, n_steps, visualize=None, fps=0.01, device=None):\n",
    "    \"\"\"Collect a fragment of the trajectory.\"\"\"\n",
    "    # (sys) get a view into numpy's observation arrays\n",
    "    vw_vis = None\n",
    "    if visualize is not None:\n",
    "        vw_vis = plyr.apply(plyr.getitem, npyt.npy.obs, index=visualize)\n",
    "\n",
    "    for j in range(n_steps):\n",
    "        if vw_vis is not None:\n",
    "            ipynb_render(vw_vis, clear=True, fps=fps)\n",
    "\n",
    "        # (sys) get $(x_t, a_{t-1}, r_t, d_t), v_t, \\pi_t$\n",
    "        _, hx, _ = out = step(env, agent, npyt, hx, device=device)\n",
    "        yield out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39226b",
   "metadata": {},
   "source": [
    "We may want to evaluate in a different env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature, _empty\n",
    "from nle_toolbox.utils.dicttools import override\n",
    "\n",
    "def_config = signature(factory).parameters['config'].default\n",
    "if def_config is _empty:\n",
    "    def_config = config\n",
    "\n",
    "eval_config = override(\n",
    "    def_config,\n",
    "    dict(\n",
    "#         id='MiniHack-CorridorBattle-Dark-MoreRats-v0',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d968e61",
   "metadata": {},
   "source": [
    "Prepare a strand extractor and a buffer for ranking episodes, and ready the vectorized env and the runtime context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9beda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epx, buf = EpisodeExtractor(), RankedBuffer(128)\n",
    "metrics = []\n",
    "\n",
    "env = SerialVecEnv(factory, n_envs=16, kwargs=dict(config=eval_config))\n",
    "npyt, hx = prepare(env, rew=0., fin=True), None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb692ae",
   "metadata": {},
   "source": [
    "A visualized evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b4b7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_total = 65536 * 4\n",
    "n_steps, visualize = 0, None\n",
    "with torch.no_grad(), tqdm.tqdm(\n",
    "    initial=n_steps, total=n_total, ncols=80,\n",
    "    disable=visualize is not None,\n",
    ") as bar:\n",
    "    nfo_ = {}\n",
    "    while progress(bar, n_steps):\n",
    "        # (sys) collect a fragment of the episode time `t` afterstates, t=0..N-1\n",
    "        fragment, hxx, nfo = zip(*collect(\n",
    "            env, agent, npyt, hx, n_steps=128,\n",
    "            visualize=visualize, fps=0.05,\n",
    "            device=device_\n",
    "        ))\n",
    "        # XXX `fragment` is ((x_t, a_{t-1}, r_t, d_t), v_t, \\mu_t), t=0..N-1\n",
    "\n",
    "        # (sys) retain running state `hx`, but detach its grads (truncated bptt)\n",
    "        # ATTN do not update `npyt` and `hx`!\n",
    "        if hxx[-1] is not None:\n",
    "            hx = plyr.apply(torch.Tensor.detach, hxx[-1])\n",
    "        \n",
    "        # (sys) shift and collate the info dicts\n",
    "        #    `d0, (d1, ..., dn)` -->> `(d0, ..., d{n-1}), dn`\n",
    "        *nfo, nfo_ = nfo_ or nfo[0], *nfo\n",
    "        nfo = plyr.apply(torch.as_tensor, *nfo, _star=False)\n",
    "\n",
    "        # (sys) repack the fragment data\n",
    "        # XXX note, `.act[t]` is $a_{t-1}$, but the other `*[t]` are $*_t$,\n",
    "        #  e.g. `.rew[t]` is $r_t$, and `pol[t]` is `$\\pi_t$.\n",
    "        input, output = plyr.apply(torch.cat, *fragment, _star=False)\n",
    "\n",
    "        # (sys) incerment the step count\n",
    "        n_steps += input.fin.numel()\n",
    "\n",
    "        # (sys) extract episode strands with log-probs of the taken actions\n",
    "        episodes = epx.extract(input.fin, Episode(input, output, nfo))\n",
    "        add_episodes(buf, episodes, C_cov=0.5)\n",
    "\n",
    "        # (evl) compute the metrics of the completed episodes\n",
    "        for ep in episodes:\n",
    "            met = ep_stats(ep)\n",
    "            if met:\n",
    "                metrics.append(met)\n",
    "\n",
    "    # (sys) extract the residual episode strands\n",
    "    unfinished = epx.finish()\n",
    "    add_episodes(buf, unfinished, C_cov=0.5)\n",
    "\n",
    "plot_ep_metrics(metrics);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83057d58",
   "metadata": {},
   "source": [
    "The stats of the episodes in the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac750ab7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "out, fps = [], None\n",
    "for rk, ep in buf:\n",
    "    npy = plyr.apply(np.asarray, plyr.apply(torch.Tensor.cpu, ep.input))\n",
    "    off = int(npy.fin[-1])  # offset for unfinished strands\n",
    "    for t in range(len(npy.fin) - off):\n",
    "        obs = plyr.apply(plyr.getitem, npy.obs, index=t)\n",
    "        ipynb_render(obs, fps=fps)\n",
    "\n",
    "    met = ep_stats(ep)\n",
    "    if met:\n",
    "        out.append(met)\n",
    "\n",
    "plot_ep_metrics(out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128b4dd",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec936527",
   "metadata": {},
   "source": [
    "Render a replay of the given episode from the agent's ego-centric view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.env.draw import draw\n",
    "\n",
    "ENV_ACTIONS = env.envs[0].unwrapped.actions\n",
    "for rk, ep in buf:\n",
    "    n_length = len(ep.input.fin) - int(ep.input.fin[-1])\n",
    "    npy = plyr.apply(lambda x: x.cpu().numpy(), ep)\n",
    "    for t in range(n_length):\n",
    "        fig = plt.figure(figsize=(6, 6), dpi=120)\n",
    "        artists = draw(fig, npy, t, actions=ENV_ACTIONS)\n",
    "        clear_output(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c64ae",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372ff9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "{k: p.grad.flatten().norm() for k, p in mot.named_parameters() if p.grad is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b49a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "{k: p.grad.flatten().norm() for k, p in agent.named_parameters() if p.grad is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf3944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03164f0b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4231d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c99984",
   "metadata": {},
   "source": [
    "Get ready to clone the successful episodes in the ranked buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58af4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_steps = 8, 64\n",
    "r_bar, c_bar = 1.01, 1.1\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac996af",
   "metadata": {},
   "source": [
    "Behaviour cloning\n",
    "* let's have a look at [Self-Imitation Learning](https://proceedings.mlr.press/v80/oh18b.html)\n",
    "> ... off-policy actor-critic algorithm that learns to reproduce the agent’s past good decisions.\n",
    "... that exploiting past good experiences can indirectly drive deep exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbffca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in tqdm.tqdm(range(50), ncols=70):\n",
    "    # sample a bath of trajectory fragments\n",
    "    input, myu, _ = out = buf.sample(n_samples, n_steps, rng=rng)\n",
    "\n",
    "    # recompute the policy and value-to-go estimates for the episode\n",
    "    # XXX amnesia training: forget the hx\n",
    "    _, vp, _ = agent(input.obs, input.act, input.rew, hx=None, fin=input.fin)\n",
    "    # XXX this is not EXACTLY identical to `fin=ep_.fin`, which is guaranteed\n",
    "    #  to contain a reset `fin[0]` and possibly a `fin[-1]` (not in case when\n",
    "    #  the episode is unfinished). We ignore pol[-1] $\\pi_{T}$ and val[-1]\n",
    "    #  $v(s_{T})$, both of which pertain to the next episode. `fin` affects\n",
    "    #  only the recurrrent state anyway and 1) we set the initial to `None`,\n",
    "    #  and 2) do not ever use the\n",
    "\n",
    "    L_loglik = pyt_polgrad(vp.pol, input.act, adv=1.)\n",
    "    ell = - L_loglik\n",
    "\n",
    "    # get the v-trace target for the critic and the advantages to pol-grad\n",
    "    # XXX here `.fin[-1]` properly blocks the last state-value backup\n",
    "#     ret, _, rho = pyt_impala(\n",
    "#         input.rew, input.fin, input.act,\n",
    "#         val['ext'], pol, myuval['ext'], myupol,\n",
    "#         gam=f_gamma['ext'], r_bar=r_bar, c_bar=c_bar\n",
    "#     )\n",
    "#     L_critic = pyt_critic(val['ext'], ret)\n",
    "\n",
    "#     ell = (L_critic * (C_critic['ext'] / 2) - L_loglik)\n",
    "\n",
    "    agent.optim.zero_grad()\n",
    "    ell.backward()\n",
    "    agent.optim.step()\n",
    "\n",
    "#     losses.append((float(L_loglik), float(L_critic)))\n",
    "    losses.append((float(L_loglik),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_loglik, = map(np.array, zip(*losses))\n",
    "\n",
    "plt.semilogy(-L_loglik, c='C1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0626949",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2687eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihack.envs import register\n",
    "from nle import nethack\n",
    "from nle.env.tasks import NetHackScore\n",
    "\n",
    "class WalkingNetHack(NetHackScore):\n",
    "    def __init__(self, *args, observation_keys, **kwargs):\n",
    "        kwargs[\"actions\"] = kwargs.pop(\n",
    "                \"actions\", tuple(nethack.CompassDirection)\n",
    "        )\n",
    "        super().__init__(*args, **kwargs, observation_keys=observation_keys)\n",
    "\n",
    "register(\n",
    "    id=\"NetHackChallengeMovingOnly-v0\",\n",
    "    entry_point=WalkingNetHack,\n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "fac = partial(factory, id='NetHackChallengeMovingOnly-v0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae1f46",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e352394",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # `(W_ii|W_if|W_ig|W_io)`\n",
    "    blsw = agent.core.weight_ih_l0[:, 128:]\n",
    "    blsw = blsw.reshape(len(blsw), -1, 5)\n",
    "    norms = blsw.norm(p=2, dim=-1)\n",
    "\n",
    "norms = dict(zip([\n",
    "    'hunger', 'status', 'hp', 'mp',\n",
    "    'str', 'dex', 'con', 'int', 'wis', 'cha',\n",
    "    'strprc', 'AC', 'encumberance',\n",
    "], norms.T))\n",
    "\n",
    "breaks = 0, 128, 256, 384, 512\n",
    "c_pair = \"C0\", \"C1\"\n",
    "fig, axes = plt.subplots(2,2, figsize=(7, 7,), dpi=300, sharey=True, sharex=True)\n",
    "for nom, ax in zip(norms, axes.flat):\n",
    "    ax.semilogy(norms[nom], label=nom)\n",
    "    for j, (a, b) in enumerate(zip(breaks[1:], breaks)):\n",
    "        ax.axvspan(a, b, color=c_pair[j&1], alpha=0.05, zorder=-10)\n",
    "\n",
    "    ax.legend(fontsize='xx-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e323d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9364a29",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
