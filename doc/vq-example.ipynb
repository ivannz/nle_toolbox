{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c51486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141868f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some multimodal 2d data\n",
    "n_samples, n_components = 512, 8\n",
    "f_scale = 0.075\n",
    "\n",
    "eps = torch.randn(n_samples, 2) * f_scale\n",
    "\n",
    "mix = torch.arange(n_samples) % n_components\n",
    "ang = mix * 2 * torch.pi / n_components\n",
    "\n",
    "c, s = torch.cos(ang), torch.sin(ang)\n",
    "R = torch.stack(\n",
    "    (\n",
    "        c,\n",
    "        s,\n",
    "        -s,\n",
    "        c,\n",
    "    ),\n",
    "    dim=-1,\n",
    ").reshape(n_samples, 2, 2)\n",
    "\n",
    "X_full = torch.bmm(R, eps.add_(1 + eps).unsqueeze_(-1)).squeeze_(-1)\n",
    "y_full = mix.clone()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=120)\n",
    "ax.scatter(*X_full.T, c=y_full)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badbd7ef",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = 0.5\n",
    "feeds_specs = {\n",
    "    \"train\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"shuffle\": True,\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"shuffle\": False,\n",
    "    },  # UNUSED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 32  # number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b64ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "perm = torch.randperm(n_samples)\n",
    "n_train = int(f_train * n_samples)\n",
    "datasets = {\n",
    "    \"train\": TensorDataset(X_full[perm[:n_train]], y_full[perm[:n_train]]),\n",
    "    \"test\": TensorDataset(X_full[perm[n_train:]], y_full[perm[n_train:]]),\n",
    "}\n",
    "\n",
    "feeds = {nom: DataLoader(datasets[nom], **spe) for nom, spe in feeds_specs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33618034",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.vq import VectorQuantizedVAE as VQ\n",
    "from nle_toolbox.zoo.vq import VQEMAUpdater, VQLossHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50762acb",
   "metadata": {},
   "source": [
    "### VQ as unsupervised denoiser, or as an online non-stationary k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from nle_toolbox.zoo.vq import VectorQuantizedVAE\n",
    "from nle_toolbox.zoo.vq import VQVAEEmbeddings, VQVAEIntegerCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f778ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.env.draw import limits  # for aesthetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "mod = nn.Sequential(\n",
    "    nn.Linear(2, 64, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 2, bias=False),\n",
    "    # vq has special output format, so we use a handy wrapper\n",
    "    VQVAEEmbeddings(VQ(num_embeddings, 2)),\n",
    "    nn.BatchNorm1d(2),\n",
    "    nn.Linear(2, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, n_components),\n",
    ").to(device)\n",
    "\n",
    "index, vqref = next(\n",
    "    filter(\n",
    "        None,\n",
    "        [\n",
    "            (j, m.wrapped) if isinstance(m, VQVAEEmbeddings) else None\n",
    "            for j, m in enumerate(mod)\n",
    "        ],\n",
    "    ),\n",
    "    (1, None),\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(mod.parameters(), lr=1e-3)\n",
    "\n",
    "# this helps us extract and pull out the vq layer-specific losses\n",
    "hlp = VQLossHelper(mod, reduction=\"sum\")\n",
    "\n",
    "# if update is False then we do not make ema updates, but use\n",
    "#  it to compute diagnostic entropy (health of the clustering)\n",
    "#  alpha is the EMA decay rate\n",
    "# XXX 1/4 seems to work best in this example, 0.5 and higher --\n",
    "#  too fast affinity switches, lower than 0.1 -- clustering is\n",
    "#  too suboptimal and slow.\n",
    "# XXX centroid which try to capture the same clusters seem to be\n",
    "#  too unstable and jittery. Could we use this to prune quantization?\n",
    "#  On the otther hand, we could try adding new centroids to decrease\n",
    "#  cluster overall clsuter variance (cluster splitting).\n",
    "ema = VQEMAUpdater(mod, alpha=0.25, update=True)\n",
    "\n",
    "eval_cpu = tuple(datasets[\"test\"].tensors)\n",
    "(*eval_device,) = map(lambda x: x.to(device), eval_cpu)\n",
    "\n",
    "hist = []\n",
    "for ep in tqdm.tqdm(range(n_epochs), ncols=50):\n",
    "    mod.train()\n",
    "    for bx, by in iter(feeds[\"train\"]):\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        with hlp, ema:\n",
    "            out = mod(bx)\n",
    "\n",
    "        logits = out.log_softmax(dim=-1)\n",
    "\n",
    "        clf_loss = F.nll_loss(logits, by)\n",
    "        vq_ell = sum(hlp.finish().values())\n",
    "\n",
    "        loss = clf_loss + vq_ell\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # in this simple example ema updates render the vq_ell `term` non diffable\n",
    "        if loss.grad_fn is not None:\n",
    "            loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        ema.step()  # if ema were updating, then this would do the work!\n",
    "\n",
    "    mod.eval()\n",
    "    with torch.no_grad():\n",
    "        out = mod(eval_device[0])\n",
    "        y_pred = out.argmax(dim=-1).cpu()\n",
    "\n",
    "        # intermediate representations\n",
    "        rep = mod[:index](eval_device[0]).cpu()\n",
    "\n",
    "        xlim, ylim = map(\n",
    "            lambda l: limits(*l),\n",
    "            zip(rep.min(0).values.tolist(), rep.max(0).values.tolist()),\n",
    "        )\n",
    "\n",
    "    hist.append(\n",
    "        tuple(ema.entropy.values())\n",
    "        + (\n",
    "            float(vq_ell),\n",
    "            float(clf_loss),\n",
    "            float((eval_cpu[1] == y_pred).float().mean()),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    sleep(0.025)\n",
    "\n",
    "    ## PLOTTING\n",
    "\n",
    "    fig, (ax, ax2) = plt.subplots(1, 2, dpi=120, figsize=(8, 2))\n",
    "    if vqref is not None:\n",
    "        try:\n",
    "            vor = Voronoi(vqref.weight.detach().cpu().numpy())\n",
    "            voronoi_plot_2d(vor, ax=ax, show_vertices=False)\n",
    "        except:\n",
    "            pass\n",
    "        ax.scatter(*vqref.weight.detach().cpu().numpy().T, s=5, color=\"C0\")\n",
    "\n",
    "    ax.scatter(\n",
    "        *rep.numpy().T,\n",
    "        c=y_pred.numpy(),  # color='magenta',\n",
    "        alpha=0.5,\n",
    "        zorder=-10,\n",
    "        s=5,\n",
    "    )\n",
    "    ax.set_aspect(1.0)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    *ents, ells, clfs, acc = map(np.array, zip(*hist))\n",
    "    if ents:\n",
    "        ax2.plot(ents[0], label=\"entropy\")\n",
    "\n",
    "    ax2_ = ax2.twinx()\n",
    "    ax2_.plot(acc, c=\"C2\", label=\"accuracy\")\n",
    "    #     ax2_.semilogy(clfs, c='C2', label='clf')\n",
    "    if ents and False:\n",
    "        ax2_.semilogy(ells, c=\"C1\", label=\"vq-loss\")\n",
    "\n",
    "    display(fig, clear=True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5cdc5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    module = mod[3].wrapped\n",
    "    input = mod[:3](bx)\n",
    "    output = module(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eeb26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d27d987a",
   "metadata": {},
   "source": [
    "compute cluster tightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed406f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    # Compute the assignments and unnormalized barycentres\n",
    "    # XXX $\\mu_k = \\frac{S_k}{n_k}$ the cluster centroid like in k-means\n",
    "    #     `size[k]` is $n_k = \\lvert j: k_j = k \\rvert$\n",
    "    #     `vecs[k]` is $S_k = \\sum_j 1_{k_j = k} x_j = n_k \\hat{\\mu}_k$\n",
    "    affinity = F.one_hot(output.indices, module.num_embeddings).to(input)\n",
    "    vecs = torch.einsum(\"...f, ...k -> kf\", input.detach(), affinity)\n",
    "    size = torch.einsum(\"...k -> k\", affinity).unsqueeze(-1)\n",
    "\n",
    "    # \\bar{\\mu} = \\frac1n \\sum_j x_j\n",
    "    bar = torch.einsum(\"...f -> f\", input.detach()) / len(input)\n",
    "\n",
    "    # n_k \\bar{\\mu}_k - n_k \\bar{\\mu}\n",
    "    dev0 = vecs.addcmul(size, bar, value=-1)\n",
    "\n",
    "    # n_k \\bar{\\mu}_k - n_k \\hat{\\mu}_k\n",
    "    dev1 = vecs.addcmul(module.weight, size, value=-1)\n",
    "\n",
    "    devs = torch.einsum(\"...f, ...f -> ...\", dev0, dev0)\n",
    "    devs += torch.einsum(\"...f, ...f -> ...\", dev1, dev1)\n",
    "\n",
    "    devs.div(size[..., 0]).nansum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d43292",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
