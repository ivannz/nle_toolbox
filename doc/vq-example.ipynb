{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c51486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141868f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some multimodal 2d data\n",
    "n_samples, n_components = 512, 8\n",
    "f_scale = 0.075\n",
    "\n",
    "eps = torch.randn(n_samples, 2) * f_scale\n",
    "\n",
    "mix = torch.arange(n_samples) % n_components\n",
    "ang = mix * 2 * torch.pi / n_components\n",
    "\n",
    "c, s = torch.cos(ang), torch.sin(ang)\n",
    "R = torch.stack(\n",
    "    (\n",
    "        c,\n",
    "        s,\n",
    "        -s,\n",
    "        c,\n",
    "    ),\n",
    "    dim=-1,\n",
    ").reshape(n_samples, 2, 2)\n",
    "\n",
    "X_full = torch.bmm(R, eps.add_(1 + eps).unsqueeze_(-1)).squeeze_(-1)\n",
    "y_full = mix.clone()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=120)\n",
    "ax.scatter(*X_full.T, c=y_full)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badbd7ef",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = 0.5\n",
    "feeds = {\n",
    "    \"train\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"shuffle\": True,\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"shuffle\": False,\n",
    "    },  # UNUSED\n",
    "}\n",
    "\n",
    "n_epochs = 256\n",
    "num_embeddings = 16  # number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b64ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "perm = torch.randperm(n_samples)\n",
    "n_train = int(f_train * n_samples)\n",
    "datasets = {\n",
    "    \"train\": TensorDataset(X_full[perm[:n_train]], y_full[perm[:n_train]]),\n",
    "    \"test\": TensorDataset(X_full[perm[n_train:]], y_full[perm[n_train:]]),\n",
    "}\n",
    "\n",
    "feeds = {nom: DataLoader(datasets[nom], **spe) for nom, spe in feeds.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33618034",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.vq import VectorQuantizedVAE as VQ\n",
    "from nle_toolbox.zoo.vq import VQEMAUpdater, VQLossHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50762acb",
   "metadata": {},
   "source": [
    "### VQ as unsupervised denoiser, or as an online non-stationary k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from nle_toolbox.zoo.vq import VectorQuantizedVAE\n",
    "from nle_toolbox.zoo.vq import VQVAEEmbeddings, VQVAEIntegerCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f778ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.env.draw import limits  # for aesthetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "mod = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    # vq has special output format, so we use a handy wrapper\n",
    "    VQVAEEmbeddings(VQ(num_embeddings, 2)),\n",
    "    nn.Linear(2, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, n_components),\n",
    ").to(device)\n",
    "\n",
    "index, vqref = next(\n",
    "    filter(\n",
    "        None,\n",
    "        [\n",
    "            (j, m.wrapped) if isinstance(m, VQVAEEmbeddings) else None\n",
    "            for j, m in enumerate(mod)\n",
    "        ],\n",
    "    ),\n",
    "    (1, None),\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(mod.parameters(), lr=1e-3)\n",
    "\n",
    "# this helps us extract and pull out the vq layer-specific losses\n",
    "hlp = VQLossHelper(mod, reduction=\"sum\")\n",
    "\n",
    "# if update is Flase then we do not make ema updates, but use\n",
    "#  it to compute diagnostic entropy (health of the clustering)\n",
    "#  alpha is the EMA decay rate\n",
    "ema = VQEMAUpdater(mod, alpha=0.25, update=True)\n",
    "\n",
    "eval_cpu = tuple(datasets[\"test\"].tensors)\n",
    "(*eval_device,) = map(lambda x: x.to(device), eval_cpu)\n",
    "\n",
    "hist = []\n",
    "for ep in tqdm.tqdm(range(n_epochs)):\n",
    "    for bx, by in iter(feeds[\"train\"]):\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        with hlp, ema:\n",
    "            out = mod(bx)\n",
    "\n",
    "        logits = out.log_softmax(dim=-1)\n",
    "\n",
    "        clf_loss = F.nll_loss(logits, by)\n",
    "        vq_ell = sum(hlp.finish().values())\n",
    "\n",
    "        loss = clf_loss + vq_ell\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # in this simple example ema updates render the vq_ell `term` non diffable\n",
    "        if loss.grad_fn is not None:\n",
    "            loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        ema.step()  # if ema were updating, then this would do the work!\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = mod(eval_device[0])\n",
    "        y_pred = out.argmax(dim=-1).cpu()\n",
    "\n",
    "        # intermediate representations\n",
    "        rep = mod[:index](eval_device[0]).cpu()\n",
    "\n",
    "        xlim, ylim = map(\n",
    "            lambda l: limits(*l),\n",
    "            zip(rep.min(0).values.tolist(), rep.max(0).values.tolist()),\n",
    "        )\n",
    "\n",
    "    hist.append(\n",
    "        tuple(ema.entropy.values())\n",
    "        + (\n",
    "            float(vq_ell),\n",
    "            float(clf_loss),\n",
    "            float((eval_cpu[1] == y_pred).float().mean()),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    sleep(0.025)\n",
    "\n",
    "    ## PLOTTING\n",
    "\n",
    "    fig, (ax, ax2) = plt.subplots(1, 2, dpi=120, figsize=(8, 2))\n",
    "    if vqref is not None:\n",
    "        try:\n",
    "            vor = Voronoi(vqref.weight.detach().cpu().numpy())\n",
    "            voronoi_plot_2d(vor, ax=ax, show_vertices=False)\n",
    "        except:\n",
    "            pass\n",
    "        ax.scatter(*vqref.weight.detach().cpu().numpy().T, s=5, color=\"C0\")\n",
    "\n",
    "    ax.scatter(\n",
    "        *rep.numpy().T,\n",
    "        c=y_pred.numpy(),  # color='magenta',\n",
    "        alpha=0.5,\n",
    "        zorder=-10,\n",
    "        s=5,\n",
    "    )\n",
    "    ax.set_aspect(1.0)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    *ents, ells, clfs, acc = map(np.array, zip(*hist))\n",
    "    if ents:\n",
    "        ax2.plot(ents[0], label=\"entropy\")\n",
    "\n",
    "    ax2_ = ax2.twinx()\n",
    "    ax2_.plot(acc, c=\"C2\", label=\"accuracy\")\n",
    "    #     ax2_.semilogy(clfs, c='C2', label='clf')\n",
    "    if ents and False:\n",
    "        ax2_.semilogy(ells, c=\"C1\", label=\"vq-loss\")\n",
    "\n",
    "    display(fig, clear=True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5cdc5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
