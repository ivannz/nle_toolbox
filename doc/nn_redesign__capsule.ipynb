{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a978e8",
   "metadata": {},
   "source": [
    "# Let's try an non-hierarchical RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import nle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del gym.Wrapper.__getattr__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1ac6e",
   "metadata": {},
   "source": [
    "Import other useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baefd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plyr\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9872ea7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9cbdf6",
   "metadata": {},
   "source": [
    "Register envs, which punishment death with `-1` reward.\n",
    "* see the base class [MiniHack](https://github.com/facebookresearch/minihack/blob/65fc16f0f321b00552ca37db8e5f850cbd369ae5/minihack/base.py#L131-L132) subclassed by `MiniHackNavigation` and `MiniHackRoom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363dc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihack.envs import register\n",
    "\n",
    "register(\n",
    "    id=\"MiniHack-Room-Ultimate-15x15-v1\",\n",
    "    entry_point=\"minihack.envs.room:MiniHackRoom15x15Ultimate\",\n",
    "    kwargs=dict(\n",
    "        reward_win=+1,  # default\n",
    "        reward_lose=-1,  # used to be 0.\n",
    "    ),\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"MiniHack-CorridorBattle-Dark-v1\",\n",
    "    entry_point=\"minihack.envs.fightcorridor:MiniHackFightCorridorDark\",\n",
    "    kwargs=dict(\n",
    "        reward_win=+1,  # default\n",
    "        reward_lose=-1,  # used to be 0.\n",
    "    ),\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"MiniHack-HideNSeek-Big-v1\",\n",
    "    entry_point=\"minihack.envs.hidenseek:MiniHackHideAndSeekBig\",\n",
    "    kwargs=dict(\n",
    "        reward_win=+1,  # default\n",
    "        reward_lose=-1,  # used to be 0.\n",
    "    ),\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"MiniHack-Memento-F4-v1\",\n",
    "    entry_point=\"minihack.envs.memento:MiniHackMementoF4\",\n",
    "    kwargs=dict(\n",
    "        reward_win=+1,  # default\n",
    "        reward_lose=-1,  # used to be 0. <<-- reward for death\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b98cdb",
   "metadata": {},
   "source": [
    "Agent's and motivator's arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab49120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.models.basic import NLENeuralAgent\n",
    "\n",
    "recipe = {\n",
    "    'agent': NLENeuralAgent.default_recipe(\n",
    "        n_actions=8,  # 85, 121  # XXX make this an automatic setting dependent on the env\n",
    "        embedding_dim=16,\n",
    "        intermediate_size=256,\n",
    "        act_embedding_dim=None,  # default to `embedding_dim`\n",
    "        fin_embedding_dim=0,  # disable `fin` flag embedding\n",
    "        core='lstm',\n",
    "\n",
    "        # it appears that more layers makes the agent learn better!\n",
    "        num_layers=2,\n",
    "        # hardcoded! see `factory()`\n",
    "        k=3,\n",
    "        bls=(\n",
    "            'hp',\n",
    "            'hunger',\n",
    "            'condition',\n",
    "        ),\n",
    "    ),\n",
    "}\n",
    "\n",
    "recipe['motivator'] = {\n",
    "    'obs': recipe['agent']['obs'],\n",
    "    'sizes': [16],\n",
    "}\n",
    "\n",
    "recipe_bas = recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffa10c",
   "metadata": {},
   "source": [
    "A recipe for Highway Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc69204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.models.transformer import NLEHITNeuralAgent\n",
    "\n",
    "recipe = {\n",
    "    'agent': NLEHITNeuralAgent.default_recipe(\n",
    "        n_actions=8,  # 121  # XXX make this an automatic setting dependent on the env\n",
    "        embedding_dim=16,\n",
    "        n_cls=8,\n",
    "        n_io=8,  # \\geq 4 is great, but \\leq 2 doesn't work\n",
    "        intermediate_size=64,\n",
    "        num_attention_heads=4,\n",
    "        head_size=16,\n",
    "\n",
    "        num_layers=1,  # 2 is gud, but can we use one layer?\n",
    "        # hardcoded! see `factory()`\n",
    "        k=3,\n",
    "        bls=(\n",
    "            'hp',\n",
    "            'hunger',\n",
    "            'condition',\n",
    "        ),\n",
    "    ),\n",
    "}\n",
    "\n",
    "recipe['motivator'] = {\n",
    "    'obs': {\n",
    "        'n_context': recipe['agent']['n_context'],\n",
    "        'embedding_dim': recipe['agent']['embedding_dim'],\n",
    "        'intermediate_size': 256,\n",
    "        'dropout': 0.0,\n",
    "    },\n",
    "    'sizes': [16],\n",
    "}\n",
    "\n",
    "recipe_hit = recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e564e",
   "metadata": {},
   "source": [
    "Pick the agent to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa048985",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # highway xformer\n",
    "    recipe, NLEAgent = recipe_hit, NLEHITNeuralAgent\n",
    "    tags = 'HiT',\n",
    "\n",
    "else:\n",
    "    recipe, NLEAgent = recipe_bas, NLENeuralAgent\n",
    "    tags = ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8f3c1",
   "metadata": {},
   "source": [
    "The fragmented a2c parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdb847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project='nle-toolbox-capsule',\n",
    "    job_type='nethack',\n",
    "    # mode='disabled',\n",
    "    config=dict(\n",
    "        # int weight in the gae mix for the polgrads\n",
    "        f_alpha=0.5,\n",
    "        f_rnd_loss_dropout=0.0,\n",
    "        f_rnd_lr=1e-3,\n",
    "\n",
    "        # extrinsic/intrinsic reward PV discount\n",
    "        f_gamma={'ext': 0.99, 'int': 0.9},\n",
    "\n",
    "        # the GAE discount\n",
    "        f_lambda={'ext': 0.96, 'int': 0.96},\n",
    "\n",
    "        # the share of the runtime state's hx's gard to be passed to h0\n",
    "        f_h0_lerp=0.00,\n",
    "\n",
    "        # critic (both ext and int) and entropy weights in the loss\n",
    "        C_pg=1.,\n",
    "        C_critic={'ext': 0.5, 'int': 0.5},\n",
    "        C_entropy=0.01,\n",
    "        f_a2c_lr=1e-3,\n",
    "\n",
    "        # the number of off-policy updates\n",
    "        n_off_policy=0,\n",
    "\n",
    "        # gradient ell-2 norm clipping\n",
    "        f_grad_norm=float('inf'),\n",
    "\n",
    "        # the truncated-bptt length rollout\n",
    "        n_fragment_length=100,\n",
    "        n_rnd_fragment_length=20,\n",
    "\n",
    "        # the number of envs run simultaneously\n",
    "        n_batch=16,\n",
    "\n",
    "        # the total number of steps allotted to training (summed across all envs)\n",
    "        n_total=2_592_000 * 1,\n",
    "\n",
    "        # also track the recipes\n",
    "        recipe=recipe,\n",
    "        \n",
    "        # the env id (which should've been here looong time ago)\n",
    "        # navigation\n",
    "#         id='MiniHack-Room-Ultimate-15x15-v1',\n",
    "#         id='MiniHack-CorridorBattle-Dark-v1',\n",
    "#         id='MiniHack-HideNSeek-Big-v1',\n",
    "#         id='MiniHack-Memento-F4-v1',\n",
    "        id='MiniHack-Memento-Short-F2-v0',\n",
    "\n",
    "        # skill acquistion (advanced), 85 actions\n",
    "#         id='MiniHack-WoD-Hard-v0',\n",
    "#         id='MiniHack-LavaCross-v0',  # 85 actions\n",
    "\n",
    "        # Full game (ultimate) 121 actions\n",
    "#         id='NetHackChallenge-v0',\n",
    "\n",
    "        b_load_ckpt=False,\n",
    "    ),\n",
    "    tags=[\n",
    "        'hx-fix',\n",
    "        *tags,\n",
    "    ],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc8a21",
   "metadata": {},
   "source": [
    "* `linear` with small TxB batch (64x5) is more noisy, and produces less `stable` policy, but converges fastest\n",
    "* `linear` with larget batch (16x100) converges a bit slower, but produces more stable policy\n",
    "* `lstm` with 16x100 converges slowest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6722cc1",
   "metadata": {},
   "source": [
    "* [MiniHack-HideNSeek-Big-v0](https://minihack.readthedocs.io/en/latest/envs/navigation/hidenseek.html)\n",
    "> ... the agent is spawned in a big room full of trees and clouds \\[, which\\] block the line of sight of the player\\[, \\] and a random monster (chosen to be more powerful than the agent). The agent, monsters and spells can pass through clouds unobstructed \\[but\\] cannot pass through trees. The goals is to make use of the environment features, avoid being seen by the monster and quickly run towards the goal.\n",
    "\n",
    "* [MiniHack-WoD-Hard-v0](https://minihack.readthedocs.io/en/latest/envs/skills/wod.html)\n",
    "> ... require mastering the usage of the wand of death (WoD). Zapping a WoD it in any direction fires a death ray which instantly kills almost any monster it hits. ... the WoD needs to be found first, only then the agent should enter the corridor with a monster (who is awake and hostile this time), kill it, and go to the staircase.\n",
    "\n",
    "* [MiniHack-LavaCross-v0](https://minihack.readthedocs.io/en/latest/envs/skills/lava_cross.html)\n",
    "> The agent can accomplish this by either levitating over it (via a potion of levitation or levitation boots) or freezing it (by zapping the wand of cold or playing the frost horn).\n",
    "\n",
    "* [MiniHack-Memento-F4-v0](https://minihack.readthedocs.io/en/latest/envs/navigation/memento.html)\n",
    "> The agent is presented with a prompt (in the form of a sleeping monster of a specific type), and then navigates along a corridor. At the end of the corridor the agent reaches a fork, and must choose a direction. One direction leads to a grid bug, which if killed terminates the episode with +1 reward. All other directions lead to failure through a invisible trap that terminates the episode when activated. The correct path is determined by the cue seen at the beginning of the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2709ece",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91212f6",
   "metadata": {},
   "source": [
    "We hide the NLE under several layers of wrappers. From the core to the shell:\n",
    "1. `ReplayToFile` handles seeding and logs the taken actions and seed into a file for later inspection and replay.\n",
    "\n",
    "2. `NLEObservationPatches` patches tty-screens, botched by the cr-lf misconfiguration of the NLE's tty term emulator and NetHacks displays (lf only).\n",
    "\n",
    "3. `NLEFeatureExtractor` adds extra features generated on-the-fly from the current NLE's observation.\n",
    "  * an ego-centric view of the specified radius into the dungeon map (`vicinity`)\n",
    "  * percentage strength (`NLE_BL_STR125`), converted to a `100`-base score, used by the game to compute extra strength bonuses.\n",
    "    * The strength stat in AD&D 2ed, upon which the mechanics of NetHack is based, comes in two ints: strength and percentage. The latter is applicable to **warrior classes** with **natural str** 18 and denotes `exceptional strength`, which confers extra chance-to-hit, damage, and chance to force locks or doors.\n",
    "\n",
    "\n",
    "4. `RecentHistory` keeps a brief log of actions taken in the environment (partially duplicates the functionality of the `Replay` wrapper).\n",
    "\n",
    "5. `Chassis` handles skippable gui events that do not require a decision, such as collecting menu pages unless an interaction is required, fetching consecutive topline or log messages.\n",
    "\n",
    "6. `ObservationDictFilter` allow only the specified keys of the observation dict to get through\n",
    "\n",
    "7. `ActionMasker` computes the mask of action that are **forbidden** in the current game state (_gui_ or _play_)\n",
    "  * the mask is communicated through the observation dicts under the key `action_mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.bot.chassis import get_wrapper\n",
    "\n",
    "from nle_toolbox.utils.replay import ReplayToFile, Replay\n",
    "\n",
    "from nle_toolbox.utils.env.wrappers import NLEObservationPatches\n",
    "from nle_toolbox.utils.env.wrappers import NLEFeatureExtractor\n",
    "from nle_toolbox.utils.env.wrappers import RecentHistory\n",
    "\n",
    "from nle_toolbox.bot.chassis import Chassis, ActionMasker\n",
    "\n",
    "from nle_toolbox.utils.env.wrappers import ObservationDictFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7194a",
   "metadata": {},
   "source": [
    "The factory for collecting random exploration rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6fe6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nle_toolbox.utils import seeding\n",
    "import minihack\n",
    "\n",
    "def factory(seed=None, folder=None, sticky=False, id=str(wandb.config.id)):\n",
    "    env = gym.make(\n",
    "        id,\n",
    "        observation_keys=(\n",
    "            'glyphs',\n",
    "            'chars',\n",
    "            'colors',\n",
    "            'specials',\n",
    "            'blstats',\n",
    "            'message',\n",
    "            'inv_glyphs',\n",
    "            'inv_strs',\n",
    "            'inv_letters',\n",
    "            'inv_oclasses',\n",
    "            'tty_chars',\n",
    "            'tty_colors',\n",
    "            'tty_cursor',\n",
    "            'misc',\n",
    "            'screen_descriptions',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    from nle.nethack import ACTIONS\n",
    "    ctoa = {chr(a): j for j, a in enumerate(env.unwrapped.actions)}\n",
    "    atoc = tuple(map(chr, env.unwrapped.actions))\n",
    "\n",
    "    # provide seeding capabilities and full action tracking\n",
    "    if folder is None:\n",
    "        env = Replay(env, sticky=sticky)\n",
    "\n",
    "    else:\n",
    "        env = ReplayToFile(env, sticky=sticky, folder=folder, save_on='done')\n",
    "    env.seed(seed)\n",
    "\n",
    "    # patch bugged tty output\n",
    "    env = NLEObservationPatches(env)\n",
    "\n",
    "    # log recent actions\n",
    "    env = RecentHistory(\n",
    "        env,\n",
    "        n_recent=128,\n",
    "        map=lambda a: atoc[a],  # XXX atoc IS NOT a dict!\n",
    "    )\n",
    "\n",
    "    # skippable gui abstraction layer. Bypassed if the action\n",
    "    # space does not bind a SPACE action.\n",
    "    env = Chassis(env, space=ctoa.get(' '), split=False)\n",
    "\n",
    "    # a feature extractor to potentially reduce the runtime complexity\n",
    "    # * ego-centric view, and properly hadnled exceptional strength stat\n",
    "    env = NLEFeatureExtractor(env, k=3)\n",
    "\n",
    "    # filter unused observation keys\n",
    "    # XXX this wrapper should be applied before any container-type\n",
    "    #  modifications of the NLE's observation space.\n",
    "    env = ObservationDictFilter(\n",
    "        env,\n",
    "        # the map, bottom line stats and inventory\n",
    "        'glyphs',\n",
    "        # 'chars',\n",
    "        # 'colors',\n",
    "        # 'specials',\n",
    "        'blstats',\n",
    "        'inv_glyphs',\n",
    "        # 'inv_strs',\n",
    "        'inv_letters',\n",
    "        # 'inv_oclasses',\n",
    "\n",
    "        # used for in-notebook rendering\n",
    "        'tty_chars',\n",
    "        'tty_colors',\n",
    "        'tty_cursor',\n",
    "\n",
    "        # used by the GUI abstraction layer (Chassis)\n",
    "        # 'message',\n",
    "        # 'misc',\n",
    "\n",
    "        # extra features produced by the upstream wrappers\n",
    "        'vicinity',\n",
    "    )\n",
    "\n",
    "    # compute and action mask based on the current NLE mode: gui or play\n",
    "    env = ActionMasker(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a3da8",
   "metadata": {},
   "source": [
    "A renderer for this **factory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7124c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "from time import sleep\n",
    "\n",
    "from nle_toolbox.utils.env.render import render as tty_render\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def ipynb_render(obs, clear=True, fps=None):\n",
    "    if fps is not None:\n",
    "        if clear:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        print(tty_render(**obs))\n",
    "        if fps > 0:\n",
    "            sleep(fps)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb157caf",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b592f56",
   "metadata": {},
   "source": [
    "### Redesigning the building blocks of the NLE featrue extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4e195",
   "metadata": {},
   "source": [
    "The design of a simple obervation encoder:\n",
    "* embed glyphs' entities and their groups additively,\n",
    "* employ the `ego` embedding: learnable offset to the centre of the vicinity (an ego-centric view into the map)\n",
    "* join with embeddings of health, hunger and condition form the botl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26525ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.models.basic import SimpleEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f8dd2",
   "metadata": {},
   "source": [
    "Intrinsic motivation via Random Network distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc808030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class RNDModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs,\n",
    "        *,\n",
    "        sizes: List[int]\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [SimpleEncoder(**obs)]\n",
    "\n",
    "        sizes = obs['intermediate_size'], *sizes\n",
    "        for n, m in zip(sizes, sizes[1:]):\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(n, m, bias=True))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, obs, act=None, rew=None, fin=None, *, hx=None):\n",
    "        return self.encoder(obs), None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1eb51",
   "metadata": {},
   "source": [
    "A clunky tool to split the parameters into biases and weights.\n",
    "\n",
    "Unlike `.bias` parameters, which **should not** be decayed, not every `.weight` parameter\n",
    "**should be** decayed. For example, learnt positional encodings can be seen as bias terms\n",
    "to the \"linear\" operation effected by an `nn.Embedding`, at the same time ordinary token\n",
    "embeddings should also not be decayed either, since they effectively serve as the input\n",
    "data representation.\n",
    "\n",
    "`nn.LayerNorm` layers perform within-layer normalization and then re-scale and translate\n",
    "it, meaning that their weight should be regularized to unit scales ant not zeros.\n",
    "\n",
    "Hence:\n",
    "* all biases, and weights of `nn.LayerNorm` and `nn.Embedding` should not be regularized\n",
    "by weight decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.models.transformer import HiT\n",
    "\n",
    "# XXX clean this up\n",
    "def split_parameters(module):\n",
    "    decay, no_decay, seen = [], [], set()\n",
    "    for prefix, mod in module.named_modules():\n",
    "        for name, par in mod.named_parameters(prefix='', recurse=False):\n",
    "            assert par not in seen\n",
    "            seen.add(par)\n",
    "\n",
    "            # always exclude bias terms\n",
    "            if name.startswith('bias'):\n",
    "                no_decay.append(par)\n",
    "\n",
    "            # but always decay dense linear weights\n",
    "            elif name.startswith('weight') and isinstance(\n",
    "                mod, (nn.Linear, nn.LSTM)\n",
    "            ):\n",
    "                decay.append(par)\n",
    "\n",
    "            # yet never decay normalization translations and embedding representations\n",
    "            elif name.startswith('weight') and isinstance(\n",
    "                mod, (nn.LayerNorm, nn.Embedding)\n",
    "            ):\n",
    "                no_decay.append(par)\n",
    "\n",
    "            elif name in ('posemb', 'cls', 'iox') and isinstance(\n",
    "                mod, (HiT,)\n",
    "            ):\n",
    "                no_decay.append(par)\n",
    "\n",
    "            else:\n",
    "                no_decay.append(par)\n",
    "                # raise TypeError(f'Unrecognized parameter `{name}` in `{prefix}` {mod}.')\n",
    "\n",
    "    return decay, no_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b500ed",
   "metadata": {},
   "source": [
    "Adam with high `weight_decay` may push many parameters' values\n",
    "into denormalized fp mode, which is ultra slow on CPU (but not\n",
    "as bad on GPU), see the answer and a reply form *njuffa* in to\n",
    "this [stackoverflow](https://stackoverflow.com/questions/36781881)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbe708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.set_flush_denormal(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd63e21",
   "metadata": {},
   "source": [
    "Build an agent and the RND motivator from the recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.nn import ModuleDict\n",
    "\n",
    "agent = NLEAgent(**wandb.config.recipe['agent'])\n",
    "\n",
    "rnd = ModuleDict(dict(\n",
    "    target=RNDModule(**wandb.config.recipe['motivator']).requires_grad_(False).eval(),\n",
    "    online=RNDModule(**wandb.config.recipe['motivator']).train(),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4e33d",
   "metadata": {},
   "source": [
    "Reset the bias terms in the recurrent core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "from nle_toolbox.utils.nn import rnn_reset_bias\n",
    "\n",
    "agent.apply(rnn_reset_bias);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f5545",
   "metadata": {},
   "source": [
    "Init agent's optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ca95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay, no_decay = split_parameters(agent)\n",
    "\n",
    "# AdamW doesn't do what you expect it to do, Ivan! (although it\n",
    "#  correctly decouples the objective's grad and the ell-2 weight reg).\n",
    "#  See https://arxiv.org/abs/1711.05101.pdf\n",
    "agent.optim = torch.optim.AdamW([\n",
    "    dict(params=decay),\n",
    "    dict(params=no_decay, weight_decay=0.),\n",
    "], lr=wandb.config.f_a2c_lr, eps=1e-5, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3ca4c",
   "metadata": {},
   "source": [
    "Load an eariler checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb.config.b_load_ckpt:\n",
    "    checkpoint = \"\"\"/Users/ivannazarov/Github/repos_with_rl/nle_toolbox/\"\"\"\\\n",
    "                 \"\"\"doc/checkpoints/ckpt__250571vf__31104000__2kfy_rb5.pt\"\"\"\n",
    "    ckpt = torch.load(checkpoint)\n",
    "    print(agent.load_state_dict(ckpt['agent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a27a7",
   "metadata": {},
   "source": [
    "An optimizer for the RND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5fa267",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay, no_decay = split_parameters(rnd.online)\n",
    "rnd.optim = torch.optim.AdamW([\n",
    "    dict(params=decay),\n",
    "    dict(params=no_decay, weight_decay=0.),\n",
    "], lr=wandb.config.f_rnd_lr, eps=1e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33113c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd53c7f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5264f",
   "metadata": {},
   "source": [
    "### Let's train an A2C agent in a capsule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c6fbc",
   "metadata": {},
   "source": [
    "An object to extract full episodes from their trajectory fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c77184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.tools import EpisodeExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28465837",
   "metadata": {},
   "source": [
    "A capulse for a learner agent and a launcher for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.capsule import launch, capsule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75236c1",
   "metadata": {},
   "source": [
    "Compute the policy gradient surrogate, the entropy and other loss components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea31c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import pyt_polgrad\n",
    "from nle_toolbox.utils.rl.engine import pyt_entropy\n",
    "from nle_toolbox.utils.rl.engine import pyt_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2ea52",
   "metadata": {},
   "source": [
    "We shall use GAE in policy gradients and returns for the critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.returns import pyt_ret_gae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7cbb1",
   "metadata": {},
   "source": [
    "Get the weighted sum of the leaves in one nested container with\n",
    "weights from another second container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad900c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator as op\n",
    "\n",
    "def reduce(values, weight=None):\n",
    "    flat = []\n",
    "    if weight is not None:\n",
    "        values = plyr.apply(op.mul, values, weight)\n",
    "\n",
    "    plyr.apply(flat.append, values)\n",
    "    return sum(flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1397a",
   "metadata": {},
   "source": [
    "A function to compute the targets (GAE, returns) for policy grads and critic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c04614",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pg_targets(rew, val, /, gam, lam, *, fin):\n",
    "    r\"\"\"Compute the targets (GAE, returns) for policy grads and critic loss.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    The arguments `rew`, `fin`, and `val` are $r_t$, $d_t$ and $v(s_t)$,\n",
    "    respectively! The td-error terms in GAE depend on $r_{t+1}$, $d_{t+1}$\n",
    "    and on both $v(s_{t+1})$ and $v(s_t)$, hence on `rew[1:]`, `fin[1:]`,\n",
    "    `val[1:]` and `val[:-1]`. `val[-1]` is the value-to-go estimate for\n",
    "    the last state in the related trajectory fragment.\n",
    "    \"\"\"\n",
    "    gae, ret = {}, {}\n",
    "    for k in rew:\n",
    "        ret[k], gae[k] = pyt_ret_gae(\n",
    "            rew[k][1:], fin[1:], val[k],\n",
    "            gam=gam[k], lam=lam[k],\n",
    "        )\n",
    "\n",
    "    return gae, ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aaf7c6",
   "metadata": {},
   "source": [
    "The Advantage (GAE) Actor-Critic loss on a fragment.\n",
    "* this one is on-policy, thus it expects the data in `vp` to be diff-able."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d991a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c_gae(input, vp, *, gam, lam, alpha):\n",
    "    \"\"\"do GAE-A2C learning w. intrinsic motivation.\"\"\"\n",
    "    # (gae) compute GAE and returns for all rewards\n",
    "    # XXX r_{t+1}, v_t, v{t+1} -->> A_t, R_t; `rew[t]`, `fin[t]`, and\n",
    "    # `val[t]` must be $r_t$, $d_t$ and $v(s_t)$, respectively, where\n",
    "    # `v_t` is computed based on the historical data $\\cdot_s$ with `s < t`!\n",
    "    gae, ret = pg_targets(input.rew, vp.val, gam, lam, fin=input.fin)\n",
    "\n",
    "    # (gae + motivation) get the weighted sum of advantages\n",
    "    adv = reduce(gae, alpha).detach()\n",
    "\n",
    "    return {\n",
    "        # (sys) policy grad surrogate `sg(G_t) \\log \\pi_t(a_t)`\n",
    "        'pg': plyr.apply(pyt_polgrad, vp.pol, input.act, adv=adv),\n",
    "        # (sys) entropy of the policy\n",
    "        'entropy': plyr.apply(pyt_entropy, vp.pol),\n",
    "        # (sys) intrinsic/extrinsic critic loss            \n",
    "        'critic': plyr.apply(pyt_critic, vp.val, ret),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f66ef",
   "metadata": {},
   "source": [
    "A helper for computing V-trace targets, that takes care of chipping\n",
    "off the initial `rew` and `fin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12674e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.returns import pyt_td_target, pyt_vtrace\n",
    "from nle_toolbox.utils.rl.returns import trailing_broadcast\n",
    "\n",
    "def pyt_vtrace_helper(rew, val, gam, *, fin, rho, r_bar, c_bar):\n",
    "    return pyt_vtrace(rew[1:], fin[1:], val, rho, gam=gam, r_bar=r_bar, c_bar=c_bar)\n",
    "\n",
    "def pyt_td_target_helper(rew, vtrace, val, gam, *, fin, rho):\n",
    "    # [O(T B F)] get the importance-weighted td(0) error advantages\n",
    "    # see sec. \"v-trace actor-critic algo\" (p. 4) in Espeholt et al. (2018)\n",
    "    target = pyt_td_target(rew[1:], fin[1:], vtrace, gam=gam)\n",
    "    return target.sub(val[:-1]).mul(trailing_broadcast(rho, rew))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846bc4b",
   "metadata": {},
   "source": [
    "A procedure to get the likelihood of actions uder a given policy sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09839037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import pyt_logpact\n",
    "\n",
    "def impala(input, vp, myu, *, gam, alpha, r_bar, c_bar):\n",
    "    # (impala) For the advantages targets, IWs and critic targets we\n",
    "    #  assume `vp` is diff-able, while `myu` isn't. Both are unstructured.\n",
    "    val_, pol_ = plyr.apply(torch.Tensor.detach, vp)\n",
    "\n",
    "    # (impala) get the importance weights\n",
    "    # XXX `act[t]` is $a_{t-1}$, pol[t] is $\\pi_t$ and $a_t \\sim \\pi_t$\n",
    "    rho = pyt_logpact(pol_, input.act) - pyt_logpact(myu.pol, input.act)\n",
    "\n",
    "    # (impala) get the V-trace targets $v_t$, t=0..N. Note that IMPALA uses\n",
    "    #  values form the behavioural policy `myu`, rather than the current\n",
    "    #  `vp`. see sec. 4.1 (p. 3) in Espeholt et al. (2018)\n",
    "    # XXX `rew[t], fin[t]` ($r_t, f_t$) PRECURSE `val[t]` ($v_t$), hence\n",
    "    # we drop the first values from `rew` and `fin`.\n",
    "    vtrace = plyr.apply(\n",
    "        pyt_vtrace_helper, input.rew, myu.val, gam, fin=input.fin,\n",
    "        rho=rho, r_bar=r_bar, c_bar=c_bar)\n",
    "\n",
    "    # (impala) get the importance-weighted td(0)-error advantages.\n",
    "    # XXX unclear if we backprop thru `vp.val` here if the `val`\n",
    "    # and `pol` networks have shared parameters.\n",
    "    adv = plyr.apply(\n",
    "        pyt_td_target_helper, input.rew, vtrace, val_, gam,\n",
    "        fin=input.fin, rho=rho.exp().clamp_(max=r_bar))\n",
    "\n",
    "    # (motivation) get the weighted sum of advantages\n",
    "    adv = reduce(adv, alpha).detach()\n",
    "    vtarget = plyr.apply(lambda t: t[:-1], vtrace)\n",
    "    return {\n",
    "        # (sys) policy grad surrogate `sg(A_t) \\log \\pi_t(a_t)`\n",
    "        'pg': pyt_polgrad(vp.pol, input.act, adv=adv),\n",
    "        # (sys) entropy of the policy\n",
    "        'entropy': plyr.apply(pyt_entropy, vp.pol),\n",
    "        # (sys) intrinsic/extrinsic critic loss\n",
    "        'critic': plyr.apply(pyt_critic, vp.val, vtarget),\n",
    "    }, rho.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5529759",
   "metadata": {},
   "source": [
    "Cache the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b764657",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = wandb.config.n_total\n",
    "n_fragment_length = wandb.config.n_fragment_length\n",
    "n_batch = wandb.config.n_batch\n",
    "\n",
    "f_lambda = wandb.config.f_lambda\n",
    "f_gamma = wandb.config.f_gamma\n",
    "f_alpha = {'ext': 1., 'int': wandb.config.f_alpha}\n",
    "\n",
    "f_h0_lerp = wandb.config.f_h0_lerp\n",
    "f_rnd_loss_dropout = wandb.config.f_rnd_loss_dropout\n",
    "\n",
    "# `-ve` maximizes, `+ve` minimizes\n",
    "f_C = {\n",
    "    'pg': -wandb.config.C_pg,\n",
    "    'entropy': -wandb.config.C_entropy,\n",
    "    'critic': wandb.config.C_critic,\n",
    "}\n",
    "\n",
    "n_off_policy = wandb.config.n_off_policy\n",
    "\n",
    "f_grad_norm = wandb.config.f_grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e0e50",
   "metadata": {},
   "source": [
    "Progress bar update and termination condition checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def progress(bar, n):\n",
    "    bar.update(n - bar.n)\n",
    "    return n < bar.total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc338825",
   "metadata": {},
   "source": [
    "A service function to get diagnostic stats and exploration metrics from an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c335d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nle.nethack import NLE_BL_SCORE\n",
    "from nle_toolbox.utils.env.defs import MAX_ENTITY, GLYPH_CMAP_OFF, symbol\n",
    "from nle.env.base import NLE\n",
    "\n",
    "from collections import namedtuple\n",
    "Episode = namedtuple('Episode', 'input,output,info')\n",
    "\n",
    "def ep_stats(ep, *, S_stone=symbol.S_stone + GLYPH_CMAP_OFF):\n",
    "    assert isinstance(ep, Episode)\n",
    "\n",
    "    met = {}\n",
    "    # determine the offset for unfinished episodes\n",
    "    offset = int(ep.input.fin[-1])\n",
    "    if len(ep.input.fin) <= offset:\n",
    "        return met\n",
    "\n",
    "    # convert to numpy `npy = plyr.apply(np.asarray, ep)`\n",
    "    # episode duration and total score\n",
    "    n_length = len(ep.input.fin) - offset\n",
    "\n",
    "    # score the episode (return = \\sum_j r_{j+1} = sum r[j], j=1..N-1)\n",
    "    f_return = plyr.apply(lambda r: r[1:].sum(), ep.input.rew)\n",
    "    f_score = ep.input.obs['blstats'][-1-offset, NLE_BL_SCORE]\n",
    "\n",
    "    # count the number of unique glyphs seen during the episode\n",
    "    vic = ep.input.obs['vicinity'][:(-1 if offset > 0 else None)]\n",
    "    cnt = torch.bincount(vic.flatten(), minlength=MAX_ENTITY + 1)\n",
    "    n_unique = cnt.gt(0).sum()\n",
    "\n",
    "    # The empirical entropy is a fine proxy for the diversity, since\n",
    "    # it measures the amount of information content the seen glyphs.\n",
    "    proba = cnt.div(cnt.sum())\n",
    "    f_entropy = F.kl_div(proba.new_zeros(()), proba, reduction='sum').neg()\n",
    "\n",
    "    # coverage and action effectiveness\n",
    "    gly = ep.input.obs['glyphs'][:(-1 if offset > 0 else None)]\n",
    "    # XXX we exclude the terminal obs, because it is actually the init\n",
    "    #  obs from the next episode\n",
    "    non_stone = (gly != S_stone).float().mean((-2, -1))\n",
    "    f_cov = non_stone.max() / non_stone.min()\n",
    "    f_eff = sum((g0 != g1).sum() / g1.numel() for g0, g1 in zip(gly, gly[1:]))\n",
    "    \n",
    "    # inspect the last info dict of the episode\n",
    "    b_death = np.nan\n",
    "    if ep.info:\n",
    "        nfo = plyr.apply(lambda x: x[-1].item(), ep.info)\n",
    "\n",
    "        # indicate if the episode ended in agent's death\n",
    "        b_death = float(nfo['end_status'] == NLE.StepStatus.DEATH)\n",
    "\n",
    "    return {\n",
    "        'duration': n_length,\n",
    "        'return': plyr.apply(float, f_return),\n",
    "        'score': int(f_score),\n",
    "        'n_unique': int(n_unique),\n",
    "        'diversity': float(f_entropy) / math.log(2),\n",
    "        'coverage': float(f_cov),\n",
    "        'effectiveness': float(f_eff),\n",
    "        'b_death': b_death,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd7e26",
   "metadata": {},
   "source": [
    "Aggregate the diagnostic stats across several episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ep_aggregate(episodes):\n",
    "    # filter out length one episodes\n",
    "    metrics = list(filter(bool, map(ep_stats, episodes)))\n",
    "    if not metrics:\n",
    "        return {}\n",
    "\n",
    "    # share of episodes that ended in agent's death\n",
    "    f_deaths = np.mean([m['b_death'] for m in metrics])\n",
    "    metrics = plyr.apply(np.median, *metrics, _star=False)\n",
    "\n",
    "    metrics['b_death'] = f_deaths\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8edbf0a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d43ff",
   "metadata": {},
   "source": [
    "Create the vectorized env and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0623ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import SerialVecEnv\n",
    "\n",
    "env = SerialVecEnv(factory, n_envs=wandb.config.n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b6608",
   "metadata": {},
   "source": [
    "How to:\n",
    "* do logging and capsule learning?\n",
    "* organize the intrinsic motivator module?\n",
    "  * ims are like regular actors, except their `actions` are rewards!\n",
    "```python\n",
    "    class SelfSupervisedRewards(nn.Module):\n",
    "        def forward(self, obs, act=None, rew=None, fin=None, *, hx=None):\n",
    "            return rew, (), hx\n",
    "```\n",
    "  * let's make intrinsict motivation modules be truly __exonegeous__ wrt the agents\n",
    "* set up the optimizers for various submodules?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401647fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator as op\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class BaseLearner(nn.Module):\n",
    "    def __init__(self, agent):\n",
    "        super().__init__()\n",
    "        self.agent = agent\n",
    "        self.epx = EpisodeExtractor()\n",
    "\n",
    "    def forward(self, input, *, hx=None):\n",
    "        return self.agent(**input._asdict(), hx=hx)\n",
    "\n",
    "    def learn(self, input, vp, *, gx=None, hx=None, nfo=None):\n",
    "        # (sys) do GAE-A2C learning w. intrinsic motivation `vp` is (v_t, \\pi_t)\n",
    "        terms = a2c_gae(input, vp, gam=f_gamma, lam=f_lambda, alpha=f_alpha)\n",
    "\n",
    "        # (sys) compute the loss\n",
    "        loss = reduce(terms, f_C)\n",
    "\n",
    "        # (sys) backprop through the agent and the online network of RND\n",
    "        self.agent.optim.zero_grad(True)\n",
    "        loss.backward()\n",
    "        grad = clip_grad_norm_(self.parameters(), 5.)\n",
    "        self.agent.optim.step()\n",
    "\n",
    "        # (sys) extra batch steps thru the current fragment with IMPALA\n",
    "        myu = plyr.apply(torch.Tensor.detach, vp)\n",
    "        for _ in range(n_off_policy):\n",
    "            _, vp, _ = self(input, hx=gx)\n",
    "            terms_, rho_ = impala(input, vp, myu, gam=f_gamma,\n",
    "                                  alpha=f_alpha, r_bar=1., c_bar=1.)\n",
    "\n",
    "            self.agent.optim.zero_grad(True)\n",
    "            reduce(terms_, f_C).backward()\n",
    "            grad = clip_grad_norm_(self.parameters(), f_grad_norm)\n",
    "            self.agent.optim.step()\n",
    "\n",
    "        # (sys) extract episode strands (drop the last record, due to overlap)\n",
    "        vw_main = plyr.apply(lambda x: x[:-1], input)\n",
    "        episodes = self.epx.extract(vw_main.fin, Episode(\n",
    "            vw_main,\n",
    "            (),\n",
    "            plyr.apply(torch.as_tensor, *nfo, _star=False),\n",
    "        ))\n",
    "\n",
    "        # (sys) recompute the recurrent state `hx` AFTER the update over the\n",
    "        # proper part of the fragment (t=0..N-1)\n",
    "        if hx is not None:\n",
    "            h0 = self.agent.initial_hx\n",
    "            # `hx` is $h_N$ from $h_{t+1}, y_t = F(z_t, h_t; w)$, t=0..N-1.\n",
    "            # We assume that one of update's side effects is the recurrent\n",
    "            # state in `hx` stale. We should recompute `hx` over the entire\n",
    "            # historical trajectory, however, since we do not store it whole,\n",
    "            # the next best thing is to make a second pass over the just\n",
    "            # collected fragment $(z_t)_{t=0}^{N-1}$ and use $h'_N$ as the new\n",
    "            # `hx`, where $h'_{t+1}, y_t = F(z_t, h'_t; w')$ with $h'_0 = h_0$\n",
    "            # given by `gx`.\n",
    "            with torch.no_grad():\n",
    "                _, _, hx = self(vw_main, hx=gx)\n",
    "\n",
    "            # DO NOT backprop through `hx` form the next fragment (truncated bptt)\n",
    "            hx = plyr.apply(torch.Tensor.detach, hx)\n",
    "            if h0 is not None and f_h0_lerp > 0:\n",
    "                hx = plyr.apply(torch.lerp, hx, h0, weight=f_h0_lerp)\n",
    "    \n",
    "        terms = plyr.apply(float, terms)\n",
    "        # XXX the batch-size normalization (TxB) can be carried out on\n",
    "        #  the logger's side as has been done with other metrics.\n",
    "        terms['entropy'] /= vw_main.fin.numel()\n",
    "        out = {'loss/loss': float(loss), 'loss/grad': float(grad)}\n",
    "        out.update({'loss/' + k: v for k, v in terms.items()})\n",
    "\n",
    "        # report diagnostic stats for completed episodes\n",
    "        out.update({'metrics/' + k: v for k, v in ep_aggregate(episodes).items()})\n",
    "\n",
    "        return out, hx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eec5c1",
   "metadata": {},
   "source": [
    "RND motivator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c233c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNDMotivator(nn.Module):\n",
    "    def __init__(self, rnd, *, f_dropout=0.0, max=None):\n",
    "        super().__init__()\n",
    "        self.rnd = rnd\n",
    "\n",
    "        self.max = max\n",
    "        self.f_dropout = f_dropout\n",
    "\n",
    "    def forward(self, input, *, hx=None):\n",
    "        # (rnd) compute the diff-able intrinsic reward\n",
    "        #     r^I_t = \\ell(f(x_t), \\bar{f}(x_t)), t=0..N\n",
    "        online, _ = self.rnd.online(**input._asdict(), hx=hx)\n",
    "        with torch.no_grad():\n",
    "            target, _ = self.rnd.target(**input._asdict(), hx=hx)\n",
    "\n",
    "        error = F.mse_loss(online, target, reduction='none').sum(-1)\n",
    "\n",
    "        # (rnd) intinsic rewards are non-diffable (optionally clamped)\n",
    "        rewards = error.detach().clone()\n",
    "        if self.max is not None:\n",
    "            rewards.clamp_(max=self.max)\n",
    "\n",
    "        # (api) return the rewards and the loss\n",
    "        return rewards, error, None\n",
    "\n",
    "    def learn(self, input, error, *, gx=None, hx=None, nfo=None):\n",
    "        # (rnd) get the dropped out loss\n",
    "        loss = F.dropout(error, p=self.f_dropout, training=True).sum()\n",
    "\n",
    "        self.rnd.optim.zero_grad(True)\n",
    "        loss.backward()\n",
    "        self.rnd.optim.step()\n",
    "\n",
    "        # wandb.log(, commit=False)\n",
    "        return {\n",
    "            'loss/rnd': float(loss),\n",
    "        }, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d7b46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def capture(fn, to):\n",
    "    @wraps(fn)\n",
    "    def _wrapper(*args, **kwargs):\n",
    "        nfo, _ = result = fn(*args, **kwargs)\n",
    "        to.update(nfo)\n",
    "        return result\n",
    "\n",
    "    return _wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc24591",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = BaseLearner(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4105ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "motivator = RNDMotivator(rnd, f_dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfceb44",
   "metadata": {},
   "source": [
    "learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b18c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import Input, prepare\n",
    "\n",
    "log = {}\n",
    "\n",
    "caps = capsule(\n",
    "    learner,\n",
    "    capture(learner.learn, log),\n",
    "    length=wandb.config.n_fragment_length,\n",
    ")\n",
    "\n",
    "motv = capsule(\n",
    "    motivator,\n",
    "    capture(motivator.learn, log),\n",
    "    length=wandb.config.n_rnd_fragment_length,\n",
    ")\n",
    "\n",
    "n_steps = 0\n",
    "with tqdm.tqdm(initial=n_steps, total=n_total, ncols=70, disable=False) as bar:\n",
    "    # the local runtime state for the main exec flow\n",
    "    # env is reset, and act is sampled from the env\n",
    "    npy, pyt = prepare(env, rew=0., fin=True)\n",
    "    \n",
    "    # specify the motivator and get its first reward\n",
    "    npy_mot = launch(motv, Input(npy.obs, npy.rew, npy.rew, npy.fin))\n",
    "\n",
    "    # recv the first action of the encapsulated agent\n",
    "    act = launch(caps, Input(\n",
    "        npy.obs,\n",
    "        npy.act,\n",
    "        {'ext': npy.rew, 'int': npy_mot},\n",
    "        npy.fin,\n",
    "    ))\n",
    "\n",
    "    while progress(bar, n_steps):\n",
    "        # step thru the env and write x_{t+1}, a_t, r_{t+1}, d_{t+1}\n",
    "        obs, rew, fin, nfo = env.step(act)\n",
    "        plyr.apply(np.copyto, npy, Input(obs, act, rew, fin))\n",
    "        n_steps += len(env)\n",
    "\n",
    "        # compute intrinsic motivation (self-supervised rewards)\n",
    "        rew = {'ext': rew, 'int': motv.send((obs, rew, fin, nfo))}\n",
    "\n",
    "        # decide which capsule to route the data to\n",
    "        act = caps.send((obs, rew, fin, nfo))\n",
    "\n",
    "        # (log) the training progress\n",
    "        if log:\n",
    "            wandb.log({'n_steps': n_steps, **log}, commit=True)\n",
    "            log.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nle_toolbox.utils.io import mkstemp\n",
    "\n",
    "target = os.path.abspath('./checkpoints')\n",
    "os.makedirs(target, exist_ok=True)\n",
    "\n",
    "checkpoint = mkstemp('.pt', f'ckpt__{wandb.run.id}__{n_steps}__', dir=target)\n",
    "torch.save({\n",
    "    'cls': str(NLEAgent),\n",
    "    'agent': agent.state_dict(),\n",
    "    'agent.optim': agent.optim.state_dict(),\n",
    "    'rnd': rnd.state_dict(),\n",
    "    'rnd.optim': rnd.optim.state_dict(),\n",
    "    'config': dict(wandb.config),\n",
    "}, checkpoint)\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6da432",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84422d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24083b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy.fin\n",
    "\n",
    "# we need to .send the data \n",
    "\n",
    "npy_cnt = np.zeros(fin.shape, int)\n",
    "\n",
    "npy_cnt += 1\n",
    "npy_cnt *= ~fin\n",
    "\n",
    "npy_cnt\n",
    "\n",
    "counter += fin\n",
    "\n",
    "n_steps\n",
    "\n",
    "{**obs, 'goal': torch.randn(*fin.shape, 32).numpy()}\n",
    "\n",
    "fin\n",
    "\n",
    "tuple(npy.obs)\n",
    "\n",
    "pyt.act"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afa69b85",
   "metadata": {},
   "source": [
    "import pdb ; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c71c1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "{k: p.grad.flatten().norm() for k, p in rnd.online.named_parameters() if p.grad is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80924455",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "{k: p.grad.flatten().norm() for k, p in agent.named_parameters() if p.grad is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a774a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54264b7d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02820413",
   "metadata": {},
   "source": [
    "We finally got something:\n",
    "* reducing the dim of the RND output vector from 64 to 16 was, perhaps,\n",
    "the most significant step in the direction of the agent learning at least\n",
    "something.\n",
    "* the next thing was removing the build embedding and using `condition`,\n",
    "`hunger`, and `vitals\n",
    "* the effect of positive `f_h0_lerp` has not been investigated\n",
    "  * reducing it from `.05` to `0.0` seems to **adversely** impact learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100e217",
   "metadata": {},
   "source": [
    "Next idea to try: like token + position in BERT and GPT, let's exploit additive embeddings.\n",
    "* in glyphs: entity + group embedding -- entities semantics is modulated by its group (relevant to MON, PET, RIDE, STATUE), also add special `ego`-embedding at the centre of vicinity.\n",
    "Let $g_{uv} \\in \\mathbb{G}$ be glyph at position $u, v$ relative to the `hero` (bls-x, -y coords).\n",
    "$$\n",
    "f_{uv}\n",
    "    = w_E\\bigl[\\operatorname{entity}(g_{uv})\\bigr]\n",
    "    + w_G\\bigl[\\operatorname{group}(g_{uv})\\bigr]\n",
    "    + 1_{(0,0)}{(u, v)} w_{\\mathrm{ego}}\n",
    "  \\,, $$\n",
    "where $w_\\cdot$ are game map-related embeddings\n",
    "* in blstats: modulate an `ego`-embedding by the `vitals`, `condition` and other stats' embeddings.\n",
    "  * should the the `ego`-embedding be shared between map and state?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95345e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce1f33",
   "metadata": {},
   "source": [
    "A ranked buffer for episode rollouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0fc1de",
   "metadata": {},
   "source": [
    "* what do we do with the missing `hx`? clone episodes in full?\n",
    "  * `you wake up on a cold stone floor in the middle of a vast chamber without a shred of memoery of how you got here. What do you do?` maybe it is OK to take contiguous fragments of a long episode and start with a wiped out memory.\n",
    "* do we clone just the actions, or also the value function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heapreplace, heappushpop, heappush, heappop\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "class RankedBuffer:\n",
    "    @dataclass(order=True, frozen=True, repr=False)\n",
    "    class RankedItem:\n",
    "        rank: float\n",
    "        item: Any = field(compare=False)\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, rk, it):\n",
    "        item = self.RankedItem(rk, it)\n",
    "        # push the current item\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            return heappush(self.buffer, item)\n",
    "        # ... pop the lowest-ranking one, if we exceed capacity\n",
    "        return heappushpop(self.buffer, item)\n",
    "\n",
    "    def extend(self, pairs):\n",
    "        last = None\n",
    "        for rk, it in pairs:\n",
    "            last = self.push(rk, it)\n",
    "        return last\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.buffer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.buffer[index]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ + f\"({len(self.buffer)}/{self.capacity})\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        return ((el.rank, el.item) for el in self.buffer)\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        n_samples=8,\n",
    "        n_steps=64,\n",
    "        *,\n",
    "        rng=np.random.default_rng(),\n",
    "    ):\n",
    "        # determinie the sufficiently long episodes\n",
    "        eligible = []\n",
    "        for j, ep in enumerate(self.buffer):\n",
    "            input = ep.item.input\n",
    "            dur_ = len(input.fin) - int(input.fin[-1])\n",
    "            if dur_ < n_steps:\n",
    "                continue\n",
    "\n",
    "            eligible.append((j, dur_,))\n",
    "\n",
    "        # sample starting strands from episodes\n",
    "        chunks = []\n",
    "        for i in rng.choice(len(eligible), size=n_samples):\n",
    "            k, dur_ = eligible[i]\n",
    "            j = rng.integers(dur_ - n_steps + 1)\n",
    "\n",
    "            chunk = plyr.apply(lambda t: t[j:j + n_steps],\n",
    "                               self.buffer[k].item)\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        return plyr.apply(torch.stack, *chunks, _star=False, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ad74d",
   "metadata": {},
   "source": [
    "A common routine to plot the computed rollout metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912caf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ep_metrics(metrics):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(7, 3), dpi=300)\n",
    "\n",
    "    out_ = plyr.apply(list, *metrics, _star=False)\n",
    "    for ax, (nom, val) in zip(axes.flat, out_.items()):\n",
    "        ax.hist(val, label=nom, log=nom in ('score', 'return',), bins=20)\n",
    "        ax.set_title(nom)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581980a",
   "metadata": {},
   "source": [
    "Rank the peisode and put it into a buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_episodes(buf, iterable, *, C_cov=0.1):\n",
    "    for ep in iterable:\n",
    "        met = ep_stats(ep)\n",
    "        if not met:\n",
    "            continue\n",
    "\n",
    "        rk = met['return'] + C_cov * met['coverage'] / met['duration']\n",
    "        buf.push(rk, ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10429b45",
   "metadata": {},
   "source": [
    "One step in the joint differentiable rollout collection and a procedure to collect a differentiable rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41626ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import step\n",
    "\n",
    "def collect(env, agent, npyt, hx, *, n_steps, visualize=None, fps=0.01):\n",
    "    \"\"\"Collect a fragment of the trajectory.\"\"\"\n",
    "    # (sys) get a view into numpy's observation arrays\n",
    "    vw_vis = None\n",
    "    if visualize is not None:\n",
    "        vw_vis = plyr.apply(plyr.getitem, npyt.npy.obs, index=visualize)\n",
    "\n",
    "    for j in range(n_steps):\n",
    "        if vw_vis is not None:\n",
    "            ipynb_render(vw_vis, clear=True, fps=fps)\n",
    "\n",
    "        # (sys) get $(x_t, a_{t-1}, r_t, d_t), v_t, \\pi_t$\n",
    "        _, hx, _ = out = step(env, agent, npyt, hx)\n",
    "        yield out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d968e61",
   "metadata": {},
   "source": [
    "Prepare a strand extractor and a buffer for ranking episodes, and ready the vectorized env and the runtime context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9beda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epx, buf = EpisodeExtractor(), RankedBuffer(128)\n",
    "metrics = []\n",
    "\n",
    "env = SerialVecEnv(factory, n_envs=4)\n",
    "npyt, hx = prepare(env, rew=0., fin=True), None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb692ae",
   "metadata": {},
   "source": [
    "A visualized evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = 16384\n",
    "n_steps, visualize = 0, 0\n",
    "with torch.no_grad(), tqdm.tqdm(\n",
    "    initial=n_steps, total=n_total, ncols=80,\n",
    "    disable=visualize is not None,\n",
    ") as bar:\n",
    "    nfo_ = {}\n",
    "    while progress(bar, n_steps):\n",
    "        # (sys) collect a fragment of the episode time `t` afterstates, t=0..N-1\n",
    "        fragment, hxx, nfo = zip(*collect(\n",
    "            env, agent, npyt, hx, n_steps=128,\n",
    "            visualize=visualize, fps=0.05\n",
    "        ))\n",
    "        # XXX `fragment` is ((x_t, a_{t-1}, r_t, d_t), v_t, \\mu_t), t=0..N-1\n",
    "\n",
    "        # (sys) retain running state `hx`, but detach its grads (truncated bptt)\n",
    "        # ATTN do not update `npyt` and `hx`!\n",
    "        if hxx[-1] is not None:\n",
    "            hx = plyr.apply(torch.Tensor.detach, hxx[-1])\n",
    "        \n",
    "        # (sys) shift and collate the info dicts\n",
    "        #    `d0, (d1, ..., dn)` -->> `(d0, ..., d{n-1}), dn`\n",
    "        *nfo, nfo_ = nfo_ or nfo[0], *nfo\n",
    "        nfo = plyr.apply(torch.as_tensor, *nfo, _star=False)\n",
    "\n",
    "        # (sys) repack the fragment data\n",
    "        # XXX note, `.act[t]` is $a_{t-1}$, but the other `*[t]` are $*_t$,\n",
    "        #  e.g. `.rew[t]` is $r_t$, and `pol[t]` is `$\\pi_t$.\n",
    "        input, output = plyr.apply(torch.cat, *fragment, _star=False)\n",
    "\n",
    "        # (sys) incerment the step count\n",
    "        n_steps += input.fin.numel()\n",
    "\n",
    "        # (sys) extract episode strands with log-probs of the taken actions\n",
    "        episodes = epx.extract(input.fin, Episode(input, output, nfo))\n",
    "        add_episodes(buf, episodes, C_cov=0.5)\n",
    "\n",
    "        # (evl) compute the metrics of the completed episodes\n",
    "        for ep in episodes:\n",
    "            met = ep_stats(ep)\n",
    "            if met:\n",
    "                metrics.append(met)\n",
    "\n",
    "    # (sys) extract the residual episode strands\n",
    "    unfinished = epx.finish()\n",
    "    add_episodes(buf, unfinished, C_cov=0.5)\n",
    "\n",
    "plot_ep_metrics(metrics);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83057d58",
   "metadata": {},
   "source": [
    "The stats of the episodes in the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac750ab7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "out, fps = [], None\n",
    "for rk, ep in buf:\n",
    "    npy = plyr.apply(np.asarray, ep.input)\n",
    "    off = int(npy.fin[-1])  # offset for unfinished strands\n",
    "    for t in range(len(npy.fin) - off):\n",
    "        obs = plyr.apply(plyr.getitem, npy.obs, index=t)\n",
    "        ipynb_render(obs, fps=fps)\n",
    "\n",
    "    met = ep_stats(ep)\n",
    "    if met:\n",
    "        out.append(met)\n",
    "\n",
    "plot_ep_metrics(out);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c99984",
   "metadata": {},
   "source": [
    "Get ready to clone the successful episodes in the ranked buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58af4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_steps = 8, 64\n",
    "r_bar, c_bar = 1.01, 1.1\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac996af",
   "metadata": {},
   "source": [
    "Behaviour cloning\n",
    "* let's have a look at [Self-Imitation Learning](https://proceedings.mlr.press/v80/oh18b.html)\n",
    "> ... off-policy actor-critic algorithm that learns to reproduce the agent’s past good decisions.\n",
    "... that exploiting past good experiences can indirectly drive deep exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbffca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in tqdm.tqdm(range(50), ncols=70):\n",
    "    # sample a bath of trajectory fragments\n",
    "    input, myu, _ = out = buf.sample(n_samples, n_steps, rng=rng)\n",
    "\n",
    "    # recompute the policy and value-to-go estimates for the episode\n",
    "    # XXX amnesia training: forget the hx\n",
    "    _, vp, _ = agent(input.obs, input.act, input.rew, hx=None, fin=input.fin)\n",
    "    # XXX this is not EXACTLY identical to `fin=ep_.fin`, which is guaranteed\n",
    "    #  to contain a reset `fin[0]` and possibly a `fin[-1]` (not in case when\n",
    "    #  the episode is unfinished). We ignore pol[-1] $\\pi_{T}$ and val[-1]\n",
    "    #  $v(s_{T})$, both of which pertain to the next episode. `fin` affects\n",
    "    #  only the recurrrent state anyway and 1) we set the initial to `None`,\n",
    "    #  and 2) do not ever use the\n",
    "\n",
    "    L_loglik = pyt_polgrad(vp.pol, input.act, adv=1.)\n",
    "    ell = - L_loglik\n",
    "\n",
    "    # get the v-trace target for the critic and the advantages to pol-grad\n",
    "    # XXX here `.fin[-1]` properly blocks the last state-value backup\n",
    "#     ret, _, rho = pyt_impala(\n",
    "#         input.rew, input.fin, input.act,\n",
    "#         val['ext'], pol, myuval['ext'], myupol,\n",
    "#         gam=f_gamma['ext'], r_bar=r_bar, c_bar=c_bar\n",
    "#     )\n",
    "#     L_critic = pyt_critic(val['ext'], ret)\n",
    "\n",
    "#     ell = (L_critic * (C_critic['ext'] / 2) - L_loglik)\n",
    "\n",
    "    agent.optim.zero_grad()\n",
    "    ell.backward()\n",
    "    agent.optim.step()\n",
    "\n",
    "#     losses.append((float(L_loglik), float(L_critic)))\n",
    "    losses.append((float(L_loglik),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_loglik, = map(np.array, zip(*losses))\n",
    "\n",
    "plt.semilogy(-L_loglik, c='C1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb8619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihack.envs import register\n",
    "from nle import nethack\n",
    "from nle.env.tasks import NetHackScore\n",
    "\n",
    "class WalkingNetHack(NetHackScore):\n",
    "    def __init__(self, *args, observation_keys, **kwargs):\n",
    "        kwargs[\"actions\"] = kwargs.pop(\n",
    "                \"actions\", tuple(nethack.CompassDirection)\n",
    "        )\n",
    "        super().__init__(*args, **kwargs, observation_keys=observation_keys)\n",
    "\n",
    "register(\n",
    "    id=\"NetHackChallengeMovingOnly-v0\",\n",
    "    entry_point=WalkingNetHack,\n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "fac = partial(factory, id='NetHackChallengeMovingOnly-v0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae1f46",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b6d6abe",
   "metadata": {},
   "source": [
    "import pdb ; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_max_grad = 1.\n",
    "epx = EpisodeExtractor()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a32ee25",
   "metadata": {},
   "source": [
    "# f_max_grad,\n",
    "f_h0_lerp, f_gamma, f_lambda, f_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f81a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(input, hx=None):\n",
    "    return agent(**input._asdict(), hx=hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1abe8e",
   "metadata": {},
   "source": [
    "Block grads thorugh the runtime state `hx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6488d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(hx, h0, *, alpha):\n",
    "    if hx is None:\n",
    "        return None\n",
    "\n",
    "    # (sys) DO NOT backprop through the runtime state into\n",
    "    # the previous fragment (truncated bptt)\n",
    "    hx = plyr.apply(torch.Tensor.detach, hx)\n",
    "\n",
    "    # (sys) DO pass some grad feedback into `h0` from the fragment\n",
    "    #   `.lerp: hx <<-- (1 - w) * hx + w * h0` (broadcasts correctly).\n",
    "    if h0 is not None and alpha > 0:\n",
    "        hx = plyr.apply(torch.lerp, hx, h0, weight=alpha)\n",
    "\n",
    "    return hx\n",
    "\n",
    "import operator as op\n",
    "\n",
    "def reduce(values, weight=None):\n",
    "    flat = []\n",
    "    if weight is not None:\n",
    "        values = plyr.apply(op.mul, values, weight)\n",
    "    plyr.apply(flat.append, values)\n",
    "    return sum(flat)\n",
    "\n",
    "def timeshift(state, *, shift=1):\n",
    "    \"\"\"Get current and shifted slices of nested objects.\"\"\"\n",
    "    # use `xgetitem` to let None through\n",
    "    # XXX `curr[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-H\n",
    "    curr = plyr.apply(plyr.xgetitem, state, index=slice(None, -shift))\n",
    "\n",
    "    # XXX `next[t]` = (x_{t+H}, a_{t+H-1}, r_{t+H}, d_{t+H}), t=0..T-H\n",
    "    next = plyr.apply(plyr.xgetitem, state, index=slice(+shift, None))\n",
    "\n",
    "    return curr, next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def update(input, output, hx=None):\n",
    "    \"\"\"do GAE-A2C learning w. intrinsic motivation\"\"\"\n",
    "    # (t-bptt) truncate grads thru the initial recurrent state\n",
    "    hx = truncate(hx, agent.initial_hx, alpha=f_h0_lerp)\n",
    "\n",
    "    # (sys) do a diff-able forward pass thru the agent to get\n",
    "    #  (v_t, \\pi_t), t=0..N, staring with h_0 over the recorded\n",
    "    #  fragment (x_t, a_{t-1}, r_t, f_t), t=0..N, but ignore,\n",
    "    #  the updated `hx` (h_{N+1})\n",
    "    _, (val, pol), _ = learner(input, hx=hx)\n",
    "\n",
    "    # (gae) compute GAE and returns for all rewards\n",
    "    # XXX `rew`, `fin`, `val` must be $r_t$, $d_t$ and $v(s_t)$!\n",
    "    gae, ret = pg_targets(input.rew, val, f_gamma, f_lambda, fin=input.fin)\n",
    "    \n",
    "    # (sys) policy grad surrogate (uses common gae!), entropy of\n",
    "    #  the policy, and the intrinsic/extrinsic critic loss.\n",
    "    # XXX r_{t+1}, v_t, v{t+1} -->> A_t \\log \\pi_t(a_t)\n",
    "    adv = reduce(gae, {'ext': 1., 'int': f_alpha}).detach()\n",
    "    terms = {\n",
    "        'pg': plyr.apply(pyt_polgrad, pol, input.act, adv=adv),\n",
    "        'entropy': plyr.apply(pyt_entropy, pol),\n",
    "        'critic': plyr.apply(pyt_critic, val, ret),        \n",
    "    }\n",
    "\n",
    "    # (sys) compute the loss\n",
    "    loss = reduce(terms, {\n",
    "        'pg': C_pg,\n",
    "        'entropy': C_entropy,\n",
    "        'critic': {\n",
    "            'ext': C_critic['ext'] / 2,\n",
    "            'int': C_critic['int'] / 2,\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # (sys) backprop through the agent\n",
    "    agent.optim.zero_grad()\n",
    "    loss.backward()\n",
    "    grad = clip_grad_norm_(agent.parameters(), f_max_grad)\n",
    "    agent.optim.step()\n",
    "\n",
    "    # (sys) get the primary slice of the framgent (the last one overlaps)\n",
    "    #  and extract episode strands\n",
    "    vw_main = plyr.apply(plyr.xgetitem, input, index=slice(None, -1))\n",
    "    episodes = epx.extract(vw_main.fin, vw_main)\n",
    "\n",
    "    # (sys) recompute the runtime state for the next fragment\n",
    "    with torch.no_grad():\n",
    "        _, _, hx = learner(vw_main, hx=hx)\n",
    "        # (sys) compute the new starting `hx`: passed thru the\n",
    "        # __updated__ agent and properly mixed with the new `h0`.\n",
    "        hx = truncate(hx, agent.initial_hx, alpha=f_h0_lerp)\n",
    "\n",
    "    # (sys) collect the data for logging\n",
    "    terms = plyr.apply(float, terms)\n",
    "    terms['entropy'] /= vw_main.fin.numel()\n",
    "    terms.update({\n",
    "        'loss': float(loss),\n",
    "        'grad': float(grad),\n",
    "    })\n",
    "\n",
    "    log = {'loss': terms}\n",
    "    if episodes:\n",
    "        log['metrics'] = plyr.apply(\n",
    "            np.median, *map(ep_stats, episodes), _star=False)\n",
    "\n",
    "    return hx, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.rl.engine import Input, prepare\n",
    "from nle_toolbox.utils.rl.capsule import collect, launch\n",
    "\n",
    "log = {}\n",
    "\n",
    "caps = collect(\n",
    "    learner,\n",
    "    n_fragment_length,\n",
    "    update=capture(update, to=log),\n",
    ")\n",
    "\n",
    "# the local runtime state for the main exec flow\n",
    "# env is reset, and act is sampled from the env\n",
    "npy, pyt = prepare(env, rew=0., fin=True)\n",
    "\n",
    "# recv the first action of the encapsulated agent\n",
    "act = launch(caps, Input(\n",
    "    npy.obs, npy.act, {'ext': npy.rew, 'int': npy.rew}, npy.fin\n",
    "))\n",
    "\n",
    "n_steps = 0\n",
    "while n_steps < 2*n_fragment_length * len(env):\n",
    "    # step thru the env and write x_{t+1}, a_t, r_{t+1}, d_{t+1}\n",
    "    obs, rew, fin, nfo = env.step(act)\n",
    "    plyr.apply(np.copyto, npy, Input(obs, act, rew, fin))\n",
    "    n_steps += len(env)\n",
    "\n",
    "    # compute intrinsic motivation (self-supervised rewards)\n",
    "    rew = {'ext': rew, 'int': torch.randn(rew.shape)}\n",
    "\n",
    "    # decide which capsule to route the data to\n",
    "    act = caps.send((obs, rew, fin, nfo))\n",
    "\n",
    "    if log:\n",
    "        wandb.log({'n_steps': n_steps, **log}, commit=True)\n",
    "        log.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d27ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d576457",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e352394",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # `(W_ii|W_if|W_ig|W_io)`\n",
    "    blsw = agent.core.weight_ih_l0[:, 128:]\n",
    "    blsw = blsw.reshape(len(blsw), -1, 5)\n",
    "    norms = blsw.norm(p=2, dim=-1)\n",
    "\n",
    "norms = dict(zip([\n",
    "    'hunger', 'status', 'hp', 'mp',\n",
    "    'str', 'dex', 'con', 'int', 'wis', 'cha',\n",
    "    'strprc', 'AC', 'encumberance',\n",
    "], norms.T))\n",
    "\n",
    "breaks = 0, 128, 256, 384, 512\n",
    "c_pair = \"C0\", \"C1\"\n",
    "fig, axes = plt.subplots(2,2, figsize=(7, 7,), dpi=300, sharey=True, sharex=True)\n",
    "for nom, ax in zip(norms, axes.flat):\n",
    "    ax.semilogy(norms[nom], label=nom)\n",
    "    for j, (a, b) in enumerate(zip(breaks[1:], breaks)):\n",
    "        ax.axvspan(a, b, color=c_pair[j&1], alpha=0.05, zorder=-10)\n",
    "\n",
    "    ax.legend(fontsize='xx-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e323d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ef74e61",
   "metadata": {},
   "source": [
    "import pdb ; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plyr.ragged(lambda v, C: C * v.sum(), pg, C_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = []\n",
    "plyr.ragged(lambda v, C: flat.append(C * v), pg_gae, C_pg)\n",
    "plyr.ragged(lambda v, C: flat.append(C * v), entropy, C_entropy)\n",
    "plyr.ragged(lambda v, C: flat.append(C * v), critic, C_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4529833",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eeb387",
   "metadata": {},
   "source": [
    "Remove currently unused fileds from the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4addd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.env.defs import MAX_GLYPH\n",
    "\n",
    "def filter(\n",
    "    glyphs,\n",
    "    blstats,\n",
    "    inv_letters,\n",
    "    inv_glyphs,\n",
    "    **ignore,\n",
    "):\n",
    "    return dict(\n",
    "        glyphs=glyphs,\n",
    "        blstats=blstats,\n",
    "        inv_letters=inv_letters,\n",
    "        inv_glyphs=inv_glyphs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f5491bf",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5837b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff75f63a",
   "metadata": {},
   "source": [
    "     y  k  u  \n",
    "      \\ | /   \n",
    "    h - . - l \n",
    "      / | \\   \n",
    "     b  j  n  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92540bc",
   "metadata": {},
   "source": [
    "Spell tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2268d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcaster(obs, mask, *, dir='.', ctoa):\n",
    "    yield from map(ctoa.get, f'Z{letter}{dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa03612",
   "metadata": {},
   "source": [
    "Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linger(obs, mask, n=16, *, seed=None, ctoa=None):\n",
    "    rng, j = np.random.default_rng(seed), 0\n",
    "    while not mask.all() and j < n:\n",
    "        # if we're in LINGER state, pick a random non-forbidden action\n",
    "        # XXX whelp... tilde on int8 is `two's complement`, not the `logical not`\n",
    "        act = rng.choice(*np.logical_not(mask).nonzero())\n",
    "\n",
    "        obs, mask = (yield act)\n",
    "        j += 1\n",
    "\n",
    "def search(obs, mask, n=6, *, ctoa):\n",
    "    yield from map(ctoa.get, f'{n:d}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63591f12",
   "metadata": {},
   "source": [
    "Level and dungeon mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb66165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle.nethack import (\n",
    "    NLE_BL_X,\n",
    "    NLE_BL_Y,\n",
    "    NLE_BL_DNUM,\n",
    "    NLE_BL_DLEVEL,\n",
    "    # NLE_BL_DEPTH,  # derived from DNUM and DLEVEL\n",
    "    # XXX does not uniquely identify floors,\n",
    "    #  c.f. [`depth`](./nle/src/dungeon.c#L1086-1084)\n",
    "    DUNGEON_SHAPE,\n",
    "    MAX_GLYPH,\n",
    ")\n",
    "\n",
    "from nle_toolbox.utils.env.defs import \\\n",
    "    glyph_is, dt_glyph_ext, ext_glyphlut\n",
    "\n",
    "from nle_toolbox.bot.level import Level, DungeonMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198a3d5",
   "metadata": {},
   "source": [
    "Detemine the walkability of the observed tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.utils.env.defs import symbol, GLYPH_CMAP_OFF, glyph_group, get_group\n",
    "from nle_toolbox.utils.env.defs import glyphlut, ext_glyphlut\n",
    "\n",
    "closed_doors = get_group(symbol, GLYPH_CMAP_OFF, *[\n",
    "    'S_vcdoor', 'S_hcdoor',\n",
    "    'S_vcdbridge', 'S_hcdbridge',\n",
    "])\n",
    "\n",
    "open_doors = get_group(symbol, GLYPH_CMAP_OFF, *[\n",
    "    'S_ndoor',\n",
    "    'S_vodoor', 'S_hodoor',\n",
    "    'S_vodbridge', 'S_hodbridge',\n",
    "])\n",
    "\n",
    "is_closed_door = np.isin(ext_glyphlut.id.value, np.array(list(closed_doors)))\n",
    "is_actor = np.isin(ext_glyphlut.id.group, np.array(list(glyph_group.ACTORS)))\n",
    "is_pet = ext_glyphlut.id.group == glyph_group.PET\n",
    "\n",
    "is_open_door = np.isin(ext_glyphlut.id.value, np.array(list(open_doors)))\n",
    "is_object = np.isin(ext_glyphlut.id.group, np.asarray(list(glyph_group.OBJECTS)))\n",
    "is_walkable = ext_glyphlut.is_accessible | is_open_door | is_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d80dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "traps = get_group(symbol, GLYPH_CMAP_OFF, *[\n",
    "    'S_arrow_trap',\n",
    "    'S_dart_trap',\n",
    "    'S_falling_rock_trap',\n",
    "    'S_squeaky_board',\n",
    "    'S_bear_trap',\n",
    "    'S_land_mine',\n",
    "    'S_rolling_boulder_trap',\n",
    "    'S_sleeping_gas_trap',\n",
    "    'S_rust_trap',\n",
    "    'S_fire_trap',\n",
    "    'S_pit',\n",
    "    'S_spiked_pit',\n",
    "    'S_hole',\n",
    "    'S_trap_door',\n",
    "    'S_teleportation_trap',\n",
    "    'S_level_teleporter',\n",
    "    'S_magic_portal',\n",
    "    'S_web',\n",
    "    'S_statue_trap',\n",
    "    'S_magic_trap',\n",
    "    'S_anti_magic_trap',\n",
    "    'S_polymorph_trap',\n",
    "    'S_vibrating_square',\n",
    "])\n",
    "\n",
    "is_trap = np.isin(ext_glyphlut.id.value, np.array(list(traps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177664e",
   "metadata": {},
   "source": [
    "The core of the \"smart\" dungeon explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def crawler(obs, mask, *, dir, seed=None):\n",
    "    dng = DungeonMapper()\n",
    "\n",
    "    # own random number generator\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # a simple state machine: linger <<-->> crawler\n",
    "    state, n_linger, stack = 'linger', 16, []\n",
    "    while True:\n",
    "        dng.update(obs)\n",
    "        pos = dng.level.trace[-1]\n",
    "\n",
    "        if state == 'crawl':\n",
    "            if stack:\n",
    "                plan.pop()\n",
    "                act = dir[stack.pop()]\n",
    "\n",
    "            else:\n",
    "                state, n_linger = 'linger', 16\n",
    "                continue\n",
    "\n",
    "        elif state == 'linger':\n",
    "            if n_linger > 0:\n",
    "                n_linger -= 1\n",
    "\n",
    "                # if we're in LINGER state, pick a random non-forbidden action\n",
    "                # XXX whelp... tilde on int8 is `two's complement`, not the `logical not`\n",
    "                act = rng.choice(*np.logical_not(mask).nonzero())\n",
    "\n",
    "            else:\n",
    "                lvl = dng.level\n",
    "\n",
    "                # we've run out linger moves, time to pick a random destination\n",
    "                # and go to it\n",
    "                state = 'crawl'\n",
    "\n",
    "                # get the walkability cost\n",
    "                cost = np.where(\n",
    "                    # is_walkable[lvl.bg_tiles.glyph]\n",
    "                    (is_walkable | is_pet)[lvl.bg_tiles.glyph]\n",
    "                    , .334, np.inf)\n",
    "                # XXX adjust `cost` for hard-to-pass objects?\n",
    "                cost[is_trap[lvl.bg_tiles.glyph]] = 10.\n",
    "\n",
    "                # get the shortest paths from the current position\n",
    "                value, path = dij(cost, pos)\n",
    "\n",
    "                # draw a destination, the further the better\n",
    "                prob = softmax(np.where(\n",
    "                    is_closed_door[lvl.bg_tiles.glyph],\n",
    "                    100.,\n",
    "                    np.where(\n",
    "                        np.logical_and(\n",
    "                            np.isfinite(value),\n",
    "                            np.logical_not(\n",
    "                                is_trap[lvl.bg_tiles.glyph]\n",
    "                            )\n",
    "                        ), value, -np.inf\n",
    "                    ))\n",
    "                )\n",
    "                dest = divmod(rng.choice(prob.size, p=prob.flat), prob.shape[1])\n",
    "\n",
    "                # reconstruct the path to the destination in reverse order\n",
    "                plan = list(backup(path, dest))\n",
    "                for (r1, c1), (r0, c0) in zip(plan, plan[1:]):\n",
    "                    stack.append(dir_to_ascii[r1-r0, c1-c0])\n",
    "\n",
    "                plan.pop()\n",
    "                continue\n",
    "\n",
    "        obs, mask = yield act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ec111",
   "metadata": {},
   "source": [
    "How do we want to explore?\n",
    "* open closed doors\n",
    "* explore tunnels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83512a48",
   "metadata": {},
   "source": [
    "Implementing the random dungeon crwaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c40bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dng = getgeneratorlocals(gen).get('dng')\n",
    "# dng.level.trace[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dng.level.bg_tiles.info.is_accessible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f588a",
   "metadata": {},
   "source": [
    "     y  k  u  \n",
    "      \\ | /   \n",
    "    h - . - l \n",
    "      / | \\   \n",
    "     b  j  n  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the walkability cost\n",
    "cost = np.where((\n",
    "    is_walkable\n",
    "    | is_pet\n",
    ")[obs['glyphs']], 1., np.inf)\n",
    "# XXX adjust `cost` for hard-to-pass objects?\n",
    "cost[is_trap[obs['glyphs']]] = 10.\n",
    "\n",
    "# get shroteste paths from the current position\n",
    "bls = obs['blstats']\n",
    "value, path = dij(cost, (bls[NLE_BL_Y], bls[NLE_BL_X]))\n",
    "\n",
    "prob = softmax(np.where(\n",
    "    np.logical_and(\n",
    "        np.isfinite(value),\n",
    "        np.logical_not(\n",
    "            is_trap[obs['glyphs']]\n",
    "        )\n",
    "    ), value, -np.inf\n",
    "))\n",
    "\n",
    "plt.imshow(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab898e",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6bc4b",
   "metadata": {},
   "source": [
    "Test the algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 12, 12\n",
    "\n",
    "rng = np.random.default_rng()  #248675)\n",
    "\n",
    "cost = -np.log(rng.random((21, 79)))\n",
    "# cost = np.ones((21, 79))\n",
    "cost[rng.random(cost.shape) < .5] = np.inf\n",
    "\n",
    "value, path = dij(cost, (r, c))\n",
    "\n",
    "\n",
    "# mask = is_walkable[lvl.bg_tiles.glyph] | is_walkable[lvl.stg_tiles.glyph]\n",
    "mask = np.isfinite(value)\n",
    "mask[r, c] = False  # mask the current position\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "value = np.where(value > 5, 0., -np.inf)\n",
    "prob = softmax(np.where(mask, value, -np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841157d1",
   "metadata": {},
   "source": [
    "Play around with the shortes path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = divmod(rng.choice(prob.size, p=prob.flat, ), prob.shape[1])\n",
    "\n",
    "displ = cost.copy()\n",
    "plan = list(backup(path, (r, c)))\n",
    "for ij in plan:\n",
    "    displ[ij] = 10\n",
    "displ[12, 12] = 11\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, dpi=300)\n",
    "ax.imshow(displ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = []\n",
    "for (r1, c1), (r0, c0) in zip(plan, plan[1:]):\n",
    "    commands.append(dir_to_ascii[r1-r0, c1-c0])\n",
    "\n",
    "''.join(reversed(commands))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d5208",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c03a68",
   "metadata": {},
   "source": [
    "A non-illegal random action exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5029be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from nle_toolbox.bot.chassis import get_wrapper\n",
    "\n",
    "\n",
    "def random_explore(seed=None, n_steps=1000, *, auto=False, fps=None, copy=False):\n",
    "    \"\"\"A non-illegal random action explorer.\n",
    "    \"\"\"\n",
    "    ss_pol, ss_env = np.random.SeedSequence(seed).spawn(2)\n",
    "\n",
    "    rng, j, n_linger, pf = np.random.default_rng(ss_pol), 0, 0, None\n",
    "    with factory(seed=ss_env) as env:\n",
    "        # we need access to the Chassis for additional meta state variables\n",
    "        cha = get_wrapper(env, Chassis)\n",
    "\n",
    "        # ActionMasker caches the esacpe action id\n",
    "        ESC = get_wrapper(env, ActionMasker).escape\n",
    "\n",
    "        # setup the dungeon mapper\n",
    "        dng = DungeonMapper()\n",
    "\n",
    "        # launch the episode\n",
    "        (obs, mask), fin = env.reset(), False\n",
    "        while (\n",
    "            ipynb_render(obs, clear=True, fps=fps)\n",
    "            and not (fin or j >= n_steps)\n",
    "        ):\n",
    "            # though nle reuses buffers, we do not deep copy them\n",
    "            #  delegating this to the downstream user instead\n",
    "            yield deepcopy(obs) if copy else obs\n",
    "\n",
    "            # default to immediately escaping from any menu or prompt\n",
    "            act = ESC\n",
    "            if not (cha.in_menu or cha.prompt):\n",
    "                dng.update(obs)\n",
    "\n",
    "                # if we're in LINGER state, pick a random non-forbidden action\n",
    "                # XXX whelp... tilde on int8 is `two's complement`, not the `logical not`\n",
    "                act = rng.choice(*np.logical_not(mask).nonzero())\n",
    "\n",
    "            (obs, mask), rew, fin, info = env.step(act)\n",
    "            j += 1\n",
    "\n",
    "            if fin and auto:\n",
    "                ipynb_render(obs, clear=True, fps=fps)\n",
    "                (obs, mask), fin = env.reset(), False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f035ca",
   "metadata": {},
   "source": [
    "Get a random episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from inspect import getgeneratorlocals\n",
    "episode = random_explore(\n",
    "    seed=None,\n",
    "    n_steps=256,\n",
    "    auto=False,\n",
    "    copy=True,\n",
    "    fps=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "glyphs = [next(episode)]\n",
    "# dng = getgeneratorlocals(episode).get('dng')\n",
    "\n",
    "glyphs.extend(obs['glyphs'] for obs in episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def dstination_prob(lvl, pos):\n",
    "    r, c = pos\n",
    "    dist = np.maximum(abs(lvl.bg_tiles.rc.r - r), abs(lvl.bg_tiles.rc.c - c))\n",
    "\n",
    "    mask = is_walkable[lvl.bg_tiles.glyph] | is_walkable[lvl.stg_tiles.glyph]\n",
    "    mask[r, c] = False  # mask the current position\n",
    "    return softmax(np.minimum(np.where(mask, dist, -np.inf), 5))\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "prob = dstination_prob(dng.level, dng.level.trace[-1])\n",
    "cost = np.where(prob > 0, 1., float('inf'))\n",
    "\n",
    "plt.imshow(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup(path, dest):\n",
    "    p0 = dest\n",
    "    while True:\n",
    "        p0, p1 = path[p0], p0\n",
    "        yield p1\n",
    "        if p0 is None:\n",
    "            return\n",
    "\n",
    "#         (r0, c0), (r1, c1) = p0, p1\n",
    "#         yield directions[r1-r0, c1-c0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "value, path = dij(cost, dng.level.trace[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa36671",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = value.copy()\n",
    "r, c = rng.choice(dng.level.bg_tiles.rc.flat, p=prob.flat)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, dpi=300)\n",
    "for i, j in backup(path, (r, c)):\n",
    "    val[i, j] = 0.\n",
    "\n",
    "val[r, c] = np.inf\n",
    "\n",
    "ax.imshow(val[:, 10:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdf31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31756f0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3da9480",
   "metadata": {},
   "source": [
    "from nle_toolbox.bot.chassis import get_wrapper\n",
    "\n",
    "def pathfinder(env, obs, seed=None):\n",
    "    # pick a random destination and lay a shortest path to it\n",
    "    path = deque(path_to(uxy, dst))\n",
    "\n",
    "    while path and reachable(uxy, path):\n",
    "        obs = yield path.popleft()\n",
    "\n",
    "    state = 0\n",
    "\n",
    "\n",
    "    # if we're in the LINGER state, pick a random non-forbidden action\n",
    "    if state == 0:\n",
    "        # XXX whelp... tilde uint8 flips the sign bit and is not the logical not\n",
    "        act = rng.choice(*np.logical_not(mask).nonzero())\n",
    "\n",
    "    elif state == 1:\n",
    "        state = 2\n",
    "\n",
    "    elif state == 2:\n",
    "        if path and reachable(uxy, path):\n",
    "            act = None\n",
    "\n",
    "        else:\n",
    "            # moving to the destination is complete, revert to lingering about\n",
    "            state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813f8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9364a29",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1888816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat, rearrange\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "from transformers.models.vit.modeling_vit import to_2tuple\n",
    "\n",
    "\n",
    "class NLEViTEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        n_rows, n_cols = to_2tuple(2 * config.window + 1)\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls = nn.Parameter(torch.zeros(\n",
    "            1, config.hidden_size))\n",
    "\n",
    "        self.posemb = nn.Parameter(torch.zeros(\n",
    "            1 + n_rows * n_cols, config.hidden_size))\n",
    "\n",
    "    def forward(self, input, **ignore):\n",
    "        x = rearrange(input, 'B D H W -> B (H W) D')\n",
    "\n",
    "        # cls-token and positional embedding\n",
    "        cls = repeat(self.cls.unsqueeze(0), '() N D -> B N D', B=len(x))\n",
    "        return torch.cat((cls, x), dim=1) + self.posemb\n",
    "\n",
    "\n",
    "model = ViTModel(ViTConfig(\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=1,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    window=2,\n",
    "    # image_size,\n",
    "    # patch_size,  # are ignored\n",
    "))\n",
    "\n",
    "model.embeddings = NLEViTEmbeddings(model.config)\n",
    "\n",
    "\n",
    "# obs = input.obs\n",
    "# gly = agent.features.obs(obs)\n",
    "\n",
    "# win = gly['vicinity']\n",
    "\n",
    "# size = dict(zip(\"TBCHW\", win.shape[:3]))\n",
    "# x = rearrange(win, 'T B C H W -> (T B) C H W')\n",
    "\n",
    "# out = rearrange(model(x).pooler_output, '(T B) C -> T B C', **size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c0bf2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
